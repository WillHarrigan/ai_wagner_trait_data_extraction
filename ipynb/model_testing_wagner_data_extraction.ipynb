{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pydantic-ai openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wagner Database AI Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code works best with a python version 3.10 < version < 3.12 (I am using 3.10.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Optional, List\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/Users/williamharrigan/Desktop/UH/Year_3/semester_2/wagner')\n",
    "import creds\n",
    "\n",
    "## pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "from openai.lib._pydantic import to_strict_json_schema\n",
    "from pydantic_ai import Agent\n",
    "from openai.types.chat.chat_completion_content_part_param import (\n",
    "    ChatCompletionContentPartTextParam,\n",
    "    ChatCompletionContentPartImageParam\n",
    ")\n",
    "\n",
    "from openai.types.chat.chat_completion_content_part_image_param import (\n",
    "    ImageURL\n",
    ")\n",
    "\n",
    "photo_dir = \"../training_set/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Description(str, Enum):\n",
    "    DICOTS = \"Dicots\"\n",
    "    MONOCOTS = \"Monocots\"\n",
    "    CONIFERS = \"Conifers\"\n",
    "    FERNS = \"Ferns and fern allies\"\n",
    "\n",
    "class StemHairType(str, Enum):\n",
    "    DENDRITIC = \"DENDRITIC\"\n",
    "    GLABROUS = \"GLABROUS\"\n",
    "    HIRSUTE = \"HIRSUTE\"\n",
    "    HISPID = \"HISPID\"\n",
    "    LEPIDOTE = \"LEPIDOTE\"\n",
    "    PILOSE = \"PILOSE\"\n",
    "    PUBERULENT = \"PUBERULENT\"\n",
    "    STRIGOSE = \"STRIGOSE\"\n",
    "    STELLATE = \"STELLATE\"\n",
    "    TOMENTOSE = \"TOMENTOSE\"\n",
    "    VILLOUS = \"VILLOUS\"\n",
    "    GLAUCOUS = \"GLAUCOUS\"\n",
    "    \n",
    "class FruitType(str, Enum):\n",
    "    ACHENE = \"ACHENE\"\n",
    "    AGGREGATE = \"AGGREGATE\"\n",
    "    ARTICLE = \"ARTICLE\"\n",
    "    BERRY = \"BERRY\"\n",
    "    CAPSULE = \"CAPSULE\"\n",
    "    CARYOPSIS = \"CARYOPSIS\"\n",
    "    DRUPE = \"DRUPE\"\n",
    "    FOLLICLE = \"FOLLICLE\"\n",
    "    LEGUME = \"LEGUME\"\n",
    "    MERICARP = \"MERICARP\"\n",
    "    MULTIPLE = \"MULTIPLE\"\n",
    "    NUT = \"NUT\"\n",
    "    PEPO = \"PEPO\"\n",
    "    POME = \"POME\"\n",
    "    SCHIZOCARP = \"SCHIZOCARP\"\n",
    "    SILICLE = \"SILICLE\"\n",
    "    SILIQUE = \"SILIQUE\"\n",
    "    SYCONIUM = \"SYCONIUM\"\n",
    "    \n",
    "class LeafShapeType(str, Enum):\n",
    "    ACEROSE = \"ACEROSE\"\n",
    "    AWL_SHAPED = \"AWL_SHAPED\"\n",
    "    GLADIATE = \"GLADIATE\"\n",
    "    HASTATE = \"HASTATE\"\n",
    "    CORDATE = \"CORDATE\"\n",
    "    DELTOID = \"DELTOID\"\n",
    "    LANCEOLATE = \"LANCEOLATE\"\n",
    "    LINEAR = \"LINEAR\"\n",
    "    ELLIPTIC = \"ELLIPTIC\"\n",
    "    ENSIFORM = \"ENSIFORM\"\n",
    "    LYRATE = \"LYRATE\"\n",
    "    OBCORDATE = \"OBCORDATE\"\n",
    "    FALCATE = \"FALCATE\"\n",
    "    FLABELLATE = \"FLABELLATE\"\n",
    "    OBDELTOID = \"OBDELTOID\"\n",
    "    OBELLIPTIC = \"OBELLIPTIC\"\n",
    "    OBLANCEOLATE = \"OBLANCEOLATE\"\n",
    "    OBLONG = \"OBLONG\"\n",
    "    PERFOLIATE = \"PERFOLIATE\"\n",
    "    QUADRATE = \"QUADRATE\"\n",
    "    OBOVATE = \"OBOVATE\"\n",
    "    ORBICULAR = \"ORBICULAR\"\n",
    "    RENIFORM = \"RENIFORM\"\n",
    "    RHOMBIC = \"RHOMBIC\"\n",
    "    OVAL = \"OVAL\"\n",
    "    OVATE = \"OVATE\"\n",
    "    ROTUND = \"ROTUND\"\n",
    "    SAGITTATE = \"SAGITTATE\"\n",
    "    PANDURATE = \"PANDURATE\"\n",
    "    PELTATE = \"PELTATE\"\n",
    "    SPATULATE = \"SPATULATE\"\n",
    "    SUBULATE = \"SUBULATE\"\n",
    "\n",
    "class StemHairs(str, Enum):\n",
    "    DENDRITIC = \"DENDRITIC\"\n",
    "    GLABROUS = \"GLABROUS\"\n",
    "\n",
    "class PhyllotaxyType(str, Enum):\n",
    "    ALTERNATE = \"ALTERNATE\"\n",
    "    OPPOSITE = \"OPPOSITE\"\n",
    "    WHORLED = \"WHORLED\"\n",
    "    DECUSSATE = \"DECUSSATE\"\n",
    "    DISTICHOUS = \"DISTICHOUS\"\n",
    "    EQUITANT = \"EQUITANT\"\n",
    "    TERNATE = \"TERNATE\"\n",
    "    CAULINE = \"CAULINE\"\n",
    "\n",
    "class InflorescenceType(str, Enum):\n",
    "    CATKIN = \"CATKIN\"\n",
    "    CYME = \"CYME\"\n",
    "    HEAD = \"HEAD\"\n",
    "    PANICLE = \"PANICLE\"\n",
    "    RACEME = \"RACEME\"\n",
    "    SPATHE_SPADIX = \"SPATHE_SPADIX\"\n",
    "    THYRSE = \"THYRSE\"\n",
    "    UMBEL = \"UMBEL\"\n",
    "    VERTISCILLATE = \"VERTISCILLATE\"\n",
    "    SOLITARY = \"SOLITARY\"\n",
    "    SPIKE = \"SPIKE\"\n",
    "    LANCEOLATE = \"LANCEOLATE\"\n",
    "    GLOBOSE = \"GLOBOSE\"\n",
    "    INVOLUCRE = \"INVOLUCRE\"\n",
    "    CORYMBOSE = \"CORYMBOSE\"\n",
    "    STROBILOID = \"STROBILOID\"\n",
    "\n",
    "class LeafHairType(str, Enum):\n",
    "    DENDRITIC = \"DENDRITIC\"\n",
    "    GLABROUS = \"GLABROUS\"\n",
    "    HIRSUTE = \"HIRSUTE\"\n",
    "    HISPID = \"HISPID\"\n",
    "    LEPIDOTE = \"LEPIDOTE\"\n",
    "    PILOSE = \"PILOSE\"\n",
    "    PUBERULENT = \"PUBERULENT\"\n",
    "    STRIGOSE = \"STRIGOSE\"\n",
    "    STELLATE = \"STELLATE\"\n",
    "    TOMENTOSE = \"TOMENTOSE\"\n",
    "    VILLOUS = \"VILLOUS\"\n",
    "    GLAUCOUS = \"GLAUCOUS\"\n",
    "\n",
    "class LeafType(str, Enum):\n",
    "    SIMPLE = \"SIMPLE\"\n",
    "    COMPOUND = \"COMPOUND\"\n",
    "\n",
    "class BreedingType(str, Enum):\n",
    "    MONOECIOUS = \"MONOECIOUS\"  # Male and female flowers on the same plant\n",
    "    ANDROMONOECIOUS = \"ANDROMONOECIOUS\"\n",
    "    CHASMOGAMOUS = \"CHASMOGAMOUS\"\n",
    "    DIOECIOUS = \"DIOECIOUS\"  # Separate male and female plants\n",
    "    GYNODIOECIOUS = \"GYNODIOECIOUS\"\n",
    "    POLYGAMO_MONOECIOUS = \"POLYGAMO-MONOECIOUS\"\n",
    "    POLYGAMOUS = \"POLYGAMOUS\"\n",
    "    POLYGAMO_DIOECIOUS = \"POLYGAMO-DIOECIOUS\"\n",
    "    GYNOMONOECIOUS = \"GYNOMONOECIOUS\"\n",
    "    STERILE = \"STERILE\"\n",
    "\n",
    "class LeafMarginType(str, Enum):\n",
    "    TEETH = \"TEETH\"\n",
    "    LOBED = \"LOBED\"\n",
    "    ENTIRE = \"ENTIRE\"\n",
    "\n",
    "\n",
    "class LifeFormType(str, Enum):\n",
    "    ANNUAL_HERB = \"ANNUAL_HERB\"\n",
    "    PERENNIAL_HERB = \"PERENNIAL_HERB\"\n",
    "    EPIPHYTE = \"EPIPHYTE\"\n",
    "    VINE = \"VINE\"\n",
    "    SHRUB = \"SHRUB\"\n",
    "    TREE = \"TREE\"\n",
    "\n",
    "class CorollaType(str, Enum):\n",
    "    ADNATE = \"ADNATE\"\n",
    "    BILABIATE = \"BILABIATE\"\n",
    "    CAMPANULATE = \"CAMPANULATE\"\n",
    "    CORYMBOSE = \"CORYMBOSE\"\n",
    "    CONVOLUTE = \"CONVOLUTE\"\n",
    "    CORONA = \"CORONA\"\n",
    "    CUNEATE = \"CUNEATE\"\n",
    "    CYLINDRICAL = \"CYLINDRICAL\"\n",
    "    DISK = \"DISK\"\n",
    "    DELTATE = \"DELTATE\"\n",
    "    ELLIPTIC = \"ELLIPTIC\"\n",
    "    FUNNELFORM = \"FUNNELFORM\"\n",
    "    FLABELLATE = \"FLABELLATE\"\n",
    "    FILIFORM = \"FILIFORM\"\n",
    "    HOOD = \"HOOD\"\n",
    "    IRREGULAR = \"IRREGULAR\"\n",
    "    KEEL = \"KEEL\"\n",
    "    LABELLUM = \"LABELLUM\"\n",
    "    LANCEOLATE = \"LANCEOLATE\"\n",
    "    LINEAR = \"LINEAR\"\n",
    "    LIPPED = \"LIPPED\"\n",
    "    LOBBED = \"LOBBED\"\n",
    "    OVATE = \"OVATE\"\n",
    "    OBLONG = \"OBLONG\"\n",
    "    OBCORDATE = \"OBCORDATE\"\n",
    "    OBOVATE = \"OBOVATE\"\n",
    "    OBLANCEOLATE = \"OBLANCEOLATE\"\n",
    "    ORBICULAR = \"ORBICULAR\"\n",
    "    PALATE = \"PALATE\"\n",
    "    PSEUDORACEMES = \"PSEUDORACEMES\"\n",
    "    ROTATE = \"ROTATE\"\n",
    "    RAY = \"RAY\"\n",
    "    REFLEXED = \"REFLEXED\"\n",
    "    RHOMBIC = \"RHOMBIC\"\n",
    "    RENIFORM = \"RENIFORM\"\n",
    "    SALVERFORM = \"SALVERFORM\"\n",
    "    SUBORBICULAR = \"SUBORBICULAR\"\n",
    "    SUBRHOMBIC = \"SUBRHOMBIC\"\n",
    "    SPUR = \"SPUR\"\n",
    "    SPATULATE = \"SPATULATE\"\n",
    "    SPICATE = \"SPICATE\"\n",
    "    SUBROTATE = \"SUBROTATE\"\n",
    "    STANDARD = \"STANDARD\"\n",
    "    TUBULAR = \"TUBULAR\"\n",
    "    TRIANGULAR = \"TRIANGULAR\"\n",
    "    URCEOLATE = \"URCEOLATE\"\n",
    "    UNILABIATE = \"UNILABIATE\"\n",
    "    VALVATE = \"VALVATE\"\n",
    "    VERTICIL = \"VERTICIL\"\n",
    "    ZYGOMORPHIC = \"ZYGOMORPHIC\"\n",
    "    CUP = \"CUP\"\n",
    "    UNGUICULATE = \"UNGUICULATE\"\n",
    "    CLAW = \"CLAW\"\n",
    "    FASICLE = \"FASICLE\"\n",
    "    STELLATE = \"STELLATE\"\n",
    "    SUBPANICULATE = \"SUBPANICULATE\"\n",
    "    PENTAGONAL = \"PENTAGONAL\"\n",
    "\n",
    "class OriginType(str, Enum):\n",
    "    NATURALIZED = \"NATURALIZED\"\n",
    "    INDIGENOUS = \"INDIGENOUS\"\n",
    "    ENDEMIC = \"ENDEMIC\"\n",
    "    POLYNESIAN_INTRODUCTION = \"POLYNESIAN INTRODUCTION\"\n",
    "\n",
    "class Location(str, Enum):\n",
    "    HAWAII = \"HAWAII\"\n",
    "    MAUI = \"MAUI\"\n",
    "    KAHOOLAWE = \"KAHOOLAWE\"\n",
    "    MOLOKAI = \"MOLOKAI\"\n",
    "    LANAI = \"LANAI\"\n",
    "    OAHU = \"OAHU\"\n",
    "    KAUAI = \"KAUAI\"\n",
    "    NIIHAU = \"NIIHAU\"\n",
    "    ALL_ISLANDS = \"ALL ISLANDS\"\n",
    "\n",
    "class FederalStatusType(str, Enum):\n",
    "    SPECIES_OF_CONCERN = \"SPECIES_OF_CONCERN\"\n",
    "    ENDANGERED = \"ENDANGERED\"\n",
    "    THREATENED = \"THREATENED\"\n",
    "    WITHDRAWN = \"WITHDRAWN\" \n",
    "    \n",
    "class StatusType(str, Enum):\n",
    "    NATURALIZED = \"NATURALIZED\"\n",
    "    ENDEMIC = \"ENDEMIC\"\n",
    "    RARE = \"RARE\"\n",
    "    SECURE = \"SECURE\"\n",
    "    VULNERABLE = \"VULNERABLE\"\n",
    "\n",
    "class Measurements(BaseModel):\n",
    "    min: Optional[float] = None\n",
    "    max: Optional[float] = None\n",
    "    extreme_min: Optional[float] = None\n",
    "    extreme_max: Optional[float] = None\n",
    "\n",
    "class HawaiianPlant(BaseModel):\n",
    "    # Basic Information\n",
    "    family: str = Field(..., description=\"Plant family name (should only be 1 object)\")\n",
    "    genus: str = Field(..., description=\"Plant genus name (should only be 1 object)\")\n",
    "    species: str = Field(..., description=\"Plant species name (should only be 1 object)\")\n",
    "    common_name: Optional[str] = Field(None, description=\"Common name of the plant\")\n",
    "    wagner_pg_number: Optional[str] = Field(None, description=\"Wagner book reference number\")\n",
    "    description: Optional[Description] = Field(None, description=\"Take knowledge from outside the passage to infer whether the plant is DICOTS, MONOCOTS, CONIFERS or FERNS\")\n",
    "    infraspecific_epithet: str = Field(..., description=\"The third word in the scientific name of an infraspecific taxon, following the name of the species. This applies only to formal names of plants and fungi, and not to the formal names of bacteria or animals. In the name Cannabis sativa subsp. indica, the word indica is the infraspecific epithet.\")\n",
    "\n",
    "    hawaiian_name: Optional[List[str]] = Field(None, description=\"List of Hawaiian names\")\n",
    "    \n",
    "    stem_hair_type: Optional[StemHairType] = Field(None, description=\"Type of hair on stem\")\n",
    "    \n",
    "    phyllotaxy_type: Optional[PhyllotaxyType] = Field(None, description=\"The arrangement of leaves around the stem.\")\n",
    "\n",
    "    leaf_hair_description: Optional[str] = Field(None, description=\"Description of leaf hair.\")\n",
    "    leaf_hair_upper_description: Optional[str] = Field(None, description=\"Description of Upper leaf hairs.\")\n",
    "    leaf_hair_lower_description: Optional[str] = Field(None, description=\"Description of Lower leaf hairs.\")\n",
    "    \n",
    "    breeding_type: Optional[BreedingType] = Field(None, description=\"Plant reproductive class.\")\n",
    "    \n",
    "    inflorescence_type: Optional[InflorescenceType] = Field(None, description=\"In a flowering plant, a cluster of flowers on a branch or a system of branches\")\n",
    "    \n",
    "    ray_color: Optional[str] = Field(None, description=\"Color of ray\")\n",
    "    floret_color: Optional[str] = Field(None, description=\"Color of florets\")\n",
    "    spathe_color: Optional[str] = Field(None, description=\"Color of spathe\")\n",
    "    perianth_outer_color: Optional[str] = Field(None, description=\"Color of perianth outer flower\")\n",
    "    perianth_inner_color: Optional[str] = Field(None, description=\"Color of perianth inner flower\")\n",
    "    perianth_color: Optional[str] = Field(None, description=\"Color of perianth\")\n",
    "    labellum_color: Optional[str] = Field(None, description=\"Color of labellum\")\n",
    "\n",
    "    corolla_type: Optional[CorollaType] = Field(None, description=\"Type of corolla\")\n",
    "    corolla_color: Optional[str] = Field(None, description=\"Color of corolla\")\n",
    "    staminate_corolla_type: Optional[CorollaType] = Field(None, description=\"Type of staminate corolla\")\n",
    "    pistillate_corolla_type: Optional[CorollaType] = Field(None, description=\"Type of pistillate corolla\")\n",
    "    \n",
    "    fruit_type: Optional[FruitType] = Field(None, description=\"Type of fruit\")\n",
    "    fruit_length: Optional[Measurements] = Field(None, description=\"Fruit length in millimeters\")\n",
    "    fruit_width: Optional[Measurements] = Field(None, description=\"Fruit width in millimeters\")\n",
    "    fruit_diameter: Optional[Measurements] = Field(None, description=\"Fruit diameter in millimeters\")\n",
    "    \n",
    "    ploidy: Optional[str] = Field(None, description=\"Ploidy level expressed as a function of n (e.g., 1n, 2n or 3n, etc..)\")\n",
    "    chromosome_number: Optional[int] = Field(None, description=\"The integer Number of chromosomes\")\n",
    "    average_chromosome_number: Optional[float] = Field(None, description=\"Average chromosome number\")\n",
    "    \n",
    "    origin: Optional[OriginType] = Field(None, description=\"Origin type of the plant\")\n",
    "    fed_status: Optional[FederalStatusType] = Field(None, description=\"Federal conservation status\")\n",
    "    status: Optional[StatusType] = Field(None, description=\"General status\")\n",
    "\n",
    "    life_form_type: Optional[LifeFormType] = Field(None, description=\"Growth habit or life form\")\n",
    "    leaf_type: Optional[LeafType] = Field(None, description=\"Simple or compound leaf type\")\n",
    "    leaf_shape_type: Optional[LeafShapeType] = Field(None, description=\"Shape of leaves\")\n",
    "    leaf_margin_type: Optional[LeafMarginType] = Field(None, description=\"Type of leaf margin\")\n",
    "    \n",
    "    juvenile_leaf_type: Optional[LeafType] = Field(None, description=\"Simple or compound juvenile leaf type\")\n",
    "    juvenile_leaf_shape_type: Optional[LeafShapeType] = Field(None, description=\"Shape of juvenile leaves\")\n",
    "    juvenile_leaf_margin_type: Optional[LeafMarginType] = Field(None, description=\"Type of leaf margin on juvenile plants\")\n",
    "    \n",
    "    leaflets_leaf_type: Optional[LeafType] = Field(None, description=\"Simple or compound juvenile leaf type\")\n",
    "    leaflets_leaf_shape_type: Optional[LeafShapeType] = Field(None, description=\"Shape of juvenile leaves\")\n",
    "    leaflets_leaf_margin_type: Optional[LeafMarginType] = Field(None, description=\"Type of leaf margin on juvenile plants\")\n",
    "    \n",
    "    leaf_hair_upper_type: Optional[LeafHairType] = Field(None, description=\"Type of upper leaf hairs\")\n",
    "    leaf_hair_lower_type: Optional[LeafHairType] = Field(None, description=\"Type of lower leaf hairs\")\n",
    "    leaf_hair_type: Optional[LeafHairType] = Field(None, description=\"Type of leaf hairs\")\n",
    "    juvenile_leaf_hair_type: Optional[LeafHairType] = Field(None, description=\"Type of juvenile leaf hairs\")\n",
    "    \n",
    "    island_type: Optional[List[Location]] = Field(None, description=\"Islands where the plant is found\")\n",
    "    \n",
    "    stem_height: Optional[Measurements] = Field(None, description=\"Stem or general plant height measurements in meters\")\n",
    "    leaf_length: Optional[Measurements] = Field(None, description=\"Length of leaves in millimeters\")\n",
    "    leaf_width: Optional[Measurements] = Field(None, description=\"Width of leaves in millimeters\")\n",
    "    juvenile_leaf_length: Optional[Measurements] = Field(None, description=\"Juvenile length of leaves in millimeters\")\n",
    "    juvenile_leaf_width: Optional[Measurements] = Field(None, description=\"Juvenile width of leaves in millimeters\")\n",
    "    leaflets_leaf_length: Optional[Measurements] = Field(None, description=\"Leaflets length of leaves in millimeters\")\n",
    "    leaflets_leaf_width: Optional[Measurements] = Field(None, description=\"Leaflets width of leaves in millimeters\")    \n",
    "    \n",
    "    petioles: Optional[Measurements] = Field(None, description=\"Length of petiole stalk in centimeters\") \n",
    "    staminate_inflorescence_length: Optional[Measurements] = Field(None, description=\"The measured length in millimeters of the male (pollen-producing) flower cluster. This specifically refers to catkins or other inflorescences containing only staminate (male) flowers.\")    \n",
    "    pistillate_inflorescence_length: Optional[Measurements] = Field(None, description=\"The measured length of the female (seed-producing) flower cluster. This specifically refers to inflorescences containing only pistillate (female) flowers.\")    \n",
    "    staminate_inflorescence_width: Optional[Measurements] = Field(None, description=\"The measured width in millimeters of the male (pollen-producing) flower cluster. This specifically refers to catkins or other inflorescences containing only staminate (male) flowers.\")    \n",
    "    pistillate_inflorescence_width: Optional[Measurements] = Field(None, description=\"The measured width of the female (seed-producing) flower cluster. This specifically refers to inflorescences containing only pistillate (female) flowers.\")        \n",
    "    \n",
    "    inflorescence_flower_length: Optional[Measurements] = Field(None, description=\"The length of an inflorescence flower in millimeters.\")\n",
    "    inflorescence_flower_width: Optional[Measurements] = Field(None, description=\"The width of an inflorescence flower in millimeters.\")\n",
    "    \n",
    "    flower_length: Optional[Measurements] = Field(None, description=\"Flower length in centimeters\")\n",
    "    flower_width: Optional[Measurements] = Field(None, description=\"Flower width in centimeters\")\n",
    "    \n",
    "    rachis_length: Optional[Measurements] = Field(None, description=\"Rachis length in millimeters\")\n",
    "    rachis_diameter: Optional[Measurements] = Field(None, description=\"Rachis diameter in millimeters\") \n",
    "    \n",
    "    head_length: Optional[Measurements] = Field(None, description=\"The measured length of the capitulum (flower head) in millimeters.\")\n",
    "    head_diameter: Optional[Measurements] = Field(None, description=\"The measured diameters of the capitulum (flower head) in millimeters.\")\n",
    "    \n",
    "    bur_length: Optional[Measurements] = Field(None, description=\"The measured length of the bur in millimeters.\")\n",
    "    tepal_length: Optional[Measurements] = Field(None, description=\"The measured length of the tepal in millimeters.\")\n",
    "    staminate_tepal_length: Optional[Measurements] = Field(None, description=\"The measured length of the staminate tepal in millimeters.\")\n",
    "    pistillate_tepal_length: Optional[Measurements] = Field(None, description=\"The measured length of the pistillate tepal in millimeters.\")\n",
    "      \n",
    "    ray_length: Optional[Measurements] = Field(None, description=\"The measured length of the ray in millimeters.\")\n",
    "    ray_width: Optional[Measurements] = Field(None, description=\"The measured width of the ray in millimeters.\")\n",
    "    \n",
    "    florets_length: Optional[Measurements] = Field(None, description=\"The measured length of the florets in millimeters.\")\n",
    "    \n",
    "    involucre_length: Optional[Measurements] = Field(None, description=\"Involucre length in millimeters\")\n",
    "    involucre_width: Optional[Measurements] = Field(None, description=\"Involucre width in millimeters\")\n",
    "    staminate_involucre_length: Optional[Measurements] = Field(None, description=\"Staminate involucre length in millimeters\")\n",
    "    pistilate_involucre_length: Optional[Measurements] = Field(None, description=\"Pistillate involucre length in millimeters\")   \n",
    "    \n",
    "    bract_length: Optional[Measurements] = Field(None, description=\"Bract length in millimeters\")\n",
    "    bract_width: Optional[Measurements] = Field(None, description=\"Bract width in millimeters\")\n",
    "    bract_lower_length: Optional[Measurements] = Field(None, description=\"Lower bract length in millimeters\")\n",
    "    bract_outer_length: Optional[Measurements] = Field(None, description=\"Outer bract length in millimeters\")\n",
    "    \n",
    "    bracteoles_length: Optional[Measurements] = Field(None, description=\"Bracteoles length in millimeters\")\n",
    "    bracteoles_width: Optional[Measurements] = Field(None, description=\"Bracteole width in millimeters\")\n",
    "    \n",
    "    pedicel_length: Optional[Measurements] = Field(None, description=\"Pedicel length in millimeters\")\n",
    "    pedicel_width: Optional[Measurements] = Field(None, description=\"Pedicel width in millimeters\")\n",
    "    staminate_pedicel_length: Optional[Measurements] = Field(None, description=\"Staminate pedicel length in millimeters\")\n",
    "    pistillate_pedicel_length: Optional[Measurements] = Field(None, description=\"Pistillate pedicel length in millimeters\")\n",
    "    staminate_pedicel_width: Optional[Measurements] = Field(None, description=\"Staminate pedicel width in millimeters\")\n",
    "    pistillate_pedicel_width: Optional[Measurements] = Field(None, description=\"Pistillate pedicel width in millimeters\") \n",
    "    \n",
    "    hypanthium_length: Optional[Measurements] = Field(None, description=\"Hypanthium length in millimeters\")\n",
    "    hypanthium_width: Optional[Measurements] = Field(None, description=\"Hypanthium width in millimeters\")\n",
    "    \n",
    "    peduncle_length: Optional[Measurements] = Field(None, description=\"Peduncle length in millimeters\")\n",
    "    peduncle_width: Optional[Measurements] = Field(None, description=\"Peduncle width in millimeters\")\n",
    "    staminate_peduncle_length: Optional[Measurements] = Field(None, description=\"Staminate peduncle length in millimeters\")\n",
    "    staminate_peduncle_width: Optional[Measurements] = Field(None, description=\"Staminate peduncle width in millimeters\")\n",
    "    pistillate_peduncle_length: Optional[Measurements] = Field(None, description=\"Pistillate peduncle length in millimeters\")\n",
    "    pistillate_peduncle_width: Optional[Measurements] = Field(None, description=\"Pistillate peduncle width in millimeters\")\n",
    "\n",
    "    spathe_width: Optional[Measurements] = Field(None, description=\"Spathe width dimensions in millimeters\")\n",
    "    spathe_length: Optional[Measurements] = Field(None, description=\"Spathe length dimensions in millimeters\")\n",
    "    spadix_length: Optional[Measurements] = Field(None, description=\"Spadix length dimensions in millimeters\")\n",
    "    \n",
    "    perianth_width: Optional[Measurements] = Field(None, description=\"Perianth width dimensions in millimeters\")\n",
    "    perianth_length: Optional[Measurements] = Field(None, description=\"Perianth length dimensions in millimeters\")   \n",
    "    perianth_outer_width: Optional[Measurements] = Field(None, description=\"Outer perianth width dimensions in millimeters\")\n",
    "    perianth_outer_length: Optional[Measurements] = Field(None, description=\"Outer perianth length dimensions in millimeters\") \n",
    "    perianth_inner_width: Optional[Measurements] = Field(None, description=\"Inner perianth width dimensions in millimeters\")\n",
    "    perianth_inner_length: Optional[Measurements] = Field(None, description=\"Inner perianth length dimensions in millimeters\") \n",
    "    \n",
    "    perianth_lobes_width: Optional[Measurements] = Field(None, description=\"Perianth lobes width dimensions in millimeters\")\n",
    "    perianth_lobes_length: Optional[Measurements] = Field(None, description=\"Perianth lobes length dimensions in millimeters\") \n",
    "    perianth_tube_length: Optional[Measurements] = Field(None, description=\"Perianth tube length dimensions in millimeters\") \n",
    "    pistillate_perianth_tube_length: Optional[Measurements] = Field(None, description=\"Pistillate perianth tube length dimensions in millimeters\") \n",
    "    staminate_perianth_tube_length: Optional[Measurements] = Field(None, description=\"Staminate perianth tube length dimensions in millimeters\") \n",
    "    \n",
    "    pappus_length: Optional[Measurements] = Field(None, description=\"Pappus length in millimeters\")\n",
    "    umbellet_length: Optional[Measurements] = Field(None, description=\"Umbellet length in millimeters\")\n",
    "    labellum_width: Optional[Measurements] = Field(None, description=\"Labellum width dimensions in millimeters\")\n",
    "    labellum_length: Optional[Measurements] = Field(None, description=\"Labellum length dimensions in millimeters\")\n",
    "    \n",
    "    calyx_length: Optional[Measurements] = Field(None, description=\"Calyx length in millimeters\")\n",
    "    calyx_width: Optional[Measurements] = Field(None, description=\"Calyx width in millimeters\")\n",
    "    calyx_teeth_length: Optional[Measurements] = Field(None, description=\"Calyx teeth length in millimeters\")\n",
    "    calyx_teeth_width: Optional[Measurements] = Field(None, description=\"Calyx teeth width in millimeters\")\n",
    "    calyx_lobes_length: Optional[Measurements] = Field(None, description=\"Calyx lobe length in millimeters\")\n",
    "    calyx_lobes_width: Optional[Measurements] = Field(None, description=\"Calyx lobe width in millimeters\")\n",
    "    \n",
    "    upper_calyx_length: Optional[Measurements] = Field(None, description=\"Upper calyx length in millimeters\")\n",
    "    lower_calyx_length: Optional[Measurements] = Field(None, description=\"Lower calyx length in millimeters\")\n",
    "    \n",
    "    inner_calyx_lobes_length: Optional[Measurements] = Field(None, description=\"Inner calyx lobes length in millimeters\")\n",
    "    inner_calyx_lobes_width: Optional[Measurements] = Field(None, description=\"Inner calyx lobes width in millimeters\")\n",
    "    outer_calyx_lobes_length: Optional[Measurements] = Field(None, description=\"Outer calyx lobes length in millimeters\")\n",
    "    outer_calyx_lobes_width: Optional[Measurements] = Field(None, description=\"Outer calyx lobes width in millimeters\")\n",
    "    \n",
    "    calyx_tube_length: Optional[Measurements] = Field(None, description=\"Calyx tube length in millimeters\")\n",
    "    calyx_tube_width: Optional[Measurements] = Field(None, description=\"Calyx tube width in millimeters\")\n",
    "    \n",
    "    male_calyx_length: Optional[Measurements] = Field(None, description=\"Male calyx length in millimeters\")\n",
    "    male_calyx_width: Optional[Measurements] = Field(None, description=\"Male calyx width in millimeters\")\n",
    "    \n",
    "    male_calyx_lobes_length: Optional[Measurements] = Field(None, description=\"Male calyx length in millimeters\")\n",
    "    male_calyx_lobes_width: Optional[Measurements] = Field(None, description=\"Male calyx width in millimeters\")\n",
    "    \n",
    "    female_calyx_length: Optional[Measurements] = Field(None, description=\"female calyx length in millimeters\")\n",
    "    female_calyx_width: Optional[Measurements] = Field(None, description=\"female calyx width in millimeters\")\n",
    "    \n",
    "    female_calyx_lobes_length: Optional[Measurements] = Field(None, description=\"Female calyx length in millimeters\")\n",
    "    female_calyx_lobes_width: Optional[Measurements] = Field(None, description=\"Female calyx width in millimeters\")\n",
    "    \n",
    "    male_calyx_lobes_length_inner: Optional[Measurements] = Field(None, description=\"Male calyx inner lobe length in millimeters\")\n",
    "    male_calyx_lobes_length_outer: Optional[Measurements] = Field(None, description=\"Male calyx outer length in millimeters\")\n",
    "    female_calyx_lobes_length_inner: Optional[Measurements] = Field(None, description=\"Female calyx inner lobe length in millimeters\")\n",
    "    female_calyx_lobes_length_outer: Optional[Measurements] = Field(None, description=\"Female calyx outer length in millimeters\")\n",
    "    \n",
    "    male_calyx_lobes_width_outer: Optional[Measurements] = Field(None, description=\"Male calyx outer lobe width in millimeters\")\n",
    "    male_calyx_tube_length: Optional[Measurements] = Field(None, description=\"Male calyx tube length in millimeters\")\n",
    "\n",
    "    female_calyx_lobes_width_inner: Optional[Measurements] = Field(None, description=\"Female calyx inner lobe width in millimeters\")\n",
    "    female_calyx_lobes_width_outer: Optional[Measurements] = Field(None, description=\"Female calyx outer lobe width in millimeters\")\n",
    "    female_calyx_tube_length: Optional[Measurements] = Field(None, description=\"Female calyx tube length in millimeters\")\n",
    "    \n",
    "    inner_calyx_length: Optional[Measurements] = Field(None, description=\"Inner calyx length in millimeters\")\n",
    "    outer_calyx_length: Optional[Measurements] = Field(None, description=\"Outer calyx length in millimeters\")\n",
    "\n",
    "    corolla_length: Optional[Measurements] = Field(None, description=\"Corolla length in millimeters\")\n",
    "    corolla_width: Optional[Measurements] = Field(None, description=\"Corolla width in millimeters\")\n",
    "    \n",
    "    corolla_length: Optional[Measurements] = Field(None, description=\"Corolla length in millimeters\")\n",
    "    corolla_width: Optional[Measurements] = Field(None, description=\"Corolla width in millimeters\")\n",
    "    corolla_tube_length: Optional[Measurements] = Field(None, description=\"Corolla tube length in millimeters\")\n",
    "    corolla_tube_width: Optional[Measurements] = Field(None, description=\"Corolla tube width in millimeters\")\n",
    "    corolla_lobes_length: Optional[Measurements] = Field(None, description=\"Corolla lobes length in millimeters\")\n",
    "    corolla_lobes_width: Optional[Measurements] = Field(None, description=\"Corolla lobes width in millimeters\")\n",
    "    \n",
    "    \n",
    "    upper_corolla: Optional[Measurements] = Field(None, description=\"Upper corolla length in millimeters\")\n",
    "    lower_corolla: Optional[Measurements] = Field(None, description=\"Lower corolla length in millimeters\")\n",
    "    upper_corolla_lobes_length: Optional[Measurements] = Field(None, description=\"Upper corolla lobes length in millimeters\")\n",
    "    lower_corolla_lobes_length: Optional[Measurements] = Field(None, description=\"Lower corolla lobes length in millimeters\")\n",
    "    \n",
    "    corolla_lip: Optional[Measurements] = Field(None, description=\"Corolla lip length in millimeters\")\n",
    "    \n",
    "    staminate_corolla_length: Optional[Measurements] = Field(None, description=\"Staminate corolla length in millimeters\")\n",
    "    pistillate_corolla_length: Optional[Measurements] = Field(None, description=\"Pistillate corolla length in millimeters\")\n",
    "    \n",
    "    staminate_corolla_tube_length: Optional[Measurements] = Field(None, description=\"Staminate corolla tube length in millimeters\")\n",
    "    pistillate_corolla_tube_length: Optional[Measurements] = Field(None, description=\"Pistillate corolla tube length in millimeters\")\n",
    "\n",
    "    staminate_corolla_tube_width: Optional[Measurements] = Field(None, description=\"Staminate corolla tube length in millimeters\")\n",
    "    pistillate_corolla_tube_width: Optional[Measurements] = Field(None, description=\"Pistillate corolla tube length in millimeters\")    \n",
    "    \n",
    "    female_corolla_lobes_length: Optional[Measurements] = Field(None, description=\"Female corolla lobes length in millimeters\") \n",
    "    female_corolla_lobes_width:  Optional[Measurements] = Field(None, description=\"Female corolla lobes width in millimeters\") \n",
    "    male_corrola_lobes_length: Optional[Measurements] = Field(None, description=\"Male corolla lobes length in millimeters\") \n",
    "    male_corrola_lobes_width: Optional[Measurements] = Field(None, description=\"Male corolla lobes width in millimeters\") \n",
    "    \n",
    "    fruit_length: Optional[Measurements] = Field(None, description=\"Fruit length in centimeters\") \n",
    "    fruit_width: Optional[Measurements] = Field(None, description=\"Fruit width in centimeters\") \n",
    "    fruit_diameter: Optional[Measurements] = Field(None, description=\"Fruit diameter in centimeters\") \n",
    "    \n",
    "    \n",
    "    seeds_perfruit: Optional[Measurements] = Field(None, description=\"Number of seeds per fruit\") \n",
    "    \n",
    "    seed_length: Optional[Measurements] = Field(None, description=\"Seed length in centimeters\") \n",
    "    seed_width: Optional[Measurements] = Field(None, description=\"Seed width in centimeters\") \n",
    "    seed_diameter: Optional[Measurements] = Field(None, description=\"Seed diameter in centimeters\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input to base64 format to be intepreted by extraction agents (chatbots)\n",
    "with open(f\"{photo_dir}/Acanthaceae_Dicliptera_chinensis.jpeg\", \"rb\") as image_file:\n",
    "            base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "## Setting the overall framework for how the extraction agent operates across all interactions\n",
    "system_prompt  = \"\"\"\n",
    "You are an expert plant taxonomist. Please analyze this image and return the details according to the schema provided.\n",
    "You are exhaustive; you include ALL the details mentioned. Do not make any assumptions about the data and do not try to\n",
    "interpret what is not obvious from the text. When extracting information, ensure that you return a structured response in JSON format matching the `HawaiianPlant` schema. \"\"\"\n",
    "\n",
    "## Prompt to set specific focus for each extraction agent interaction (can be dynamic as need be)\n",
    "user_prompt= \"Transcribe the plant information you see in this image\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o Text Transcription (Text Extraction Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting extraction agent to be gpt-4o\n",
    "## System prompt is being set to specifically transcribe information\n",
    "openai_img_to_text_transcription_agent = Agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    result_type=str,\n",
    "    system_prompt = \"You are a vision model capable of accurately performing OCR on an image\",\n",
    ")\n",
    "\n",
    "## Data to be extracted from input image\n",
    "image_urls = [\n",
    "    f\"data:image/png;base64,{base64_image}\",\n",
    "]\n",
    "\n",
    "## Setting chat parameters: low detail (for efficiency) from input image\n",
    "image_params = [\n",
    "    ChatCompletionContentPartImageParam(\n",
    "        type='image_url', \n",
    "        image_url=ImageURL(url=url, detail='low')\n",
    "    ) for url in image_urls\n",
    "]\n",
    "\n",
    "## Setting chat prompt to 'user_prompt'\n",
    "msg_open_ai = [\n",
    "            ChatCompletionContentPartTextParam(text=\"Convert this to text. Don't miss any text.\", type='text'),\n",
    "            *image_params\n",
    "]\n",
    "\n",
    "## Running data extraction\n",
    "r = await openai_img_to_text_transcription_agent.run(msg_open_ai)\n",
    "\n",
    "## Output to 'text'\n",
    "text = r.data.split(\"---\")[1]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek Model Extraction (R1 distill llama 70B) from text to Hawaiian Plant Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Setting deepseek LLM as extraction agent\n",
    "# ## Data to be extracted in HawaiianPlant schema\n",
    "# groq_extraction_agent = Agent(\n",
    "#     model=\"groq:deepseek-r1-distill-llama-70b\",\n",
    "#     retries=3,\n",
    "#     result_type=HawaiianPlant,\n",
    "#     system_prompt = system_prompt,\n",
    "#     # Had to raise temperature to 0.5\n",
    "#     model_settings = {'temperature': 0.5}\n",
    "# )\n",
    "\n",
    "# ## Setting variables and extracting data\n",
    "# r = await groq_extraction_agent.run(text)\n",
    "# deepseek_llama = r.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o Data Extraction to Hawaiian Plant Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting extraction agent to be gpt-4o\n",
    "## Extracted data will be formatted according to HawaiianPlant schema\n",
    "openai_extraction_agent = Agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    result_type=HawaiianPlant,\n",
    "    system_prompt = system_prompt,\n",
    ")\n",
    "\n",
    "## Data to be extracted from input image\n",
    "image_urls = [\n",
    "    f\"data:image/png;base64,{base64_image}\",\n",
    "]\n",
    "\n",
    "## Setting chat parameters: low detail (for efficiency) from input image\n",
    "image_params = [\n",
    "    ChatCompletionContentPartImageParam(\n",
    "        type='image_url', \n",
    "        image_url=ImageURL(url=url, detail='low')\n",
    "    ) for url in image_urls\n",
    "]\n",
    "\n",
    "## Setting chat prompt to 'user_prompt'\n",
    "msg_open_ai = [\n",
    "            ChatCompletionContentPartTextParam(text=user_prompt, type='text'),\n",
    "            *image_params\n",
    "]\n",
    "\n",
    "## Running data extraction\n",
    "r = await openai_extraction_agent.run(msg_open_ai)\n",
    "\n",
    "## Setting results to 'gpt_4o_output'\n",
    "gpt_4o_output = r.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic Model Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting extraction agent to be Claude Sonnet model\n",
    "## Extracted data will be formatted according to HawaiianPlant schema\n",
    "sonnet_extraction_agent = Agent(\n",
    "    model=\"anthropic:claude-3-5-sonnet-latest\",\n",
    "    result_type=HawaiianPlant,\n",
    "    system_prompt = system_prompt,\n",
    "    model_settings = {'temperature': 0.2}\n",
    "\n",
    ")\n",
    "\n",
    "## Setting context\n",
    "msg_claude = [\n",
    "    ChatCompletionContentPartTextParam(text=user_prompt, type='text'),\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": f\"{base64_image}\",\n",
    "                    }\n",
    "                },\n",
    "    \n",
    "]\n",
    "\n",
    "## Extracting data from image\n",
    "r = await sonnet_extraction_agent.run(msg_claude)\n",
    "sonnet_output = r.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT o3-Mini Model Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting system prompt\n",
    "gptmini_extraction_agent = Agent(\n",
    "    model=\"openai:o3-mini\",\n",
    "    result_type=HawaiianPlant,\n",
    "    system_prompt = system_prompt,\n",
    ")\n",
    "\n",
    "## Extracting output from extracted text\n",
    "r = await gptmini_extraction_agent.run(text)\n",
    "\n",
    "## Setting output\n",
    "gpt_mini_output = r.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Cheirodendron forbesii (Sherff) Lowry  \n",
      "[= Kaduaea Kraj. var. Forbesii Sherff]  \n",
      "(end)  \n",
      "\n",
      "Trees 3-4 m tall. Leaves 3 (occasionally 4 on juvenile leaves), subcoriaceous, elliptic to ovate, 5-12 cm long and less than 3.5 cm wide, venation sunken on upper surface in dried material (weakly so on juvenile leaves), margins entire, apex rounded-acute to acuminate, base narrowly cuneate to attenuate, petioles 3-7(-10) mm. inflorescence: umbo, pedicels 0.5-1.5 in long surpassing leaves, pubescence on calyx lobes sparse dense, styles 3-1 mm long, corl (4-5)-conic, styles (4)(5) 1-1.5 mm long, concave. Vein: the length into a conical configuration: triangular or verruculent. styles 5 in spaces.  \n",
      "Propagation: USU can be made by strong-hybridizing in water ac. Collector: Natural cycle: Prepare a cutting handling of water flow, 90-100 M beneath 110K endnote.  \n",
      "Distribution: Wet forests, 600-900 m, Mount Kahili, north fork of Kana ohe, Punoe, Malakala w. Mountains, and around Makaeo.  \n",
      "Note: Species was originally described as being named Z. Recognition by a restricted tier.  \n",
      "\n",
      "Cheirodendron kaianense var. forbesii, but now a separate recognition by authorities; Z variety. Now distinguished into 3 separate attempts by jc.  \n",
      "Cheirodendron kaianense var. forbesii K. Mont., NY: PU) was recently collected using this, and rehydrate was successfully performed toward the ends (6 months)- based on the cited range. Successfully established from one of the two reported localities from Lowe's: 4177, strikingly different from its up foil, but sufficiently covered, umbrella (over dark brown. Venation appearance varied from evacuation upward).  \n",
      "Note: The type collection was from wet montane habitat collected along the northern slope of high rail from Luxbridge 2. Collection records range to 3 cm, dark brown; oak brown, pubescent veins, in collection) b. Jones 1142 (BISH).  \n",
      "Biological Specifications: Young fruits immature to mature v.c. have yet to reach that of subspecies, subglabrous and nearing entire phylline stages.  \n",
      "\n",
      "Figures: freckles between F & M.\n",
      "Done: gpt_4o_output\n",
      "Done: sonnet_output\n",
      "Done: miniGPT \n"
     ]
    }
   ],
   "source": [
    "family = 'Araliaceae'\n",
    "genus = 'Cheirodendron'\n",
    "species = 'forbesii'\n",
    "\n",
    "## Input to base64 format to be intepreted by extraction agents (chatbots)\n",
    "with open(f\"{photo_dir}/{family}_{genus}_{species}.jpeg\", \"rb\") as image_file:\n",
    "            base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "## Setting the overall framework for how the extraction agent operates across all interactions\n",
    "system_prompt  = \"\"\"\n",
    "You are an expert plant taxonomist. Please analyze this image and return the details according to the schema provided.\n",
    "You are exhaustive; you include ALL the details mentioned. Do not make any assumptions about the data and do not try to\n",
    "interpret what is not obvious from the text. When extracting information, ensure that you return a structured response in JSON format matching the `HawaiianPlant` schema. \"\"\"\n",
    "\n",
    "## Prompt to set specific focus for each extraction agent interaction (can be dynamic as need be)\n",
    "user_prompt= \"Transcribe the plant information you see in this image\"\n",
    "\n",
    "## Setting extraction agent to be gpt-4o\n",
    "## System prompt is being set to specifically transcribe information\n",
    "openai_img_to_text_transcription_agent = Agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    result_type=str,\n",
    "    system_prompt = \"You are a vision model capable of accurately performing OCR on an image\",\n",
    ")\n",
    "\n",
    "## Data to be extracted from input image\n",
    "image_urls = [\n",
    "    f\"data:image/png;base64,{base64_image}\",\n",
    "]\n",
    "\n",
    "## Setting chat parameters: low detail (for efficiency) from input image\n",
    "image_params = [\n",
    "    ChatCompletionContentPartImageParam(\n",
    "        type='image_url', \n",
    "        image_url=ImageURL(url=url, detail='low')\n",
    "    ) for url in image_urls\n",
    "]\n",
    "\n",
    "## Setting chat prompt to 'user_prompt'\n",
    "msg_open_ai = [\n",
    "            ChatCompletionContentPartTextParam(text=\"Convert this to text. Don't miss any text.\", type='text'),\n",
    "            *image_params\n",
    "]\n",
    "\n",
    "## Running data extraction\n",
    "r = await openai_img_to_text_transcription_agent.run(msg_open_ai)\n",
    "\n",
    "## Output to 'text'\n",
    "\n",
    "if len(r.data.split(\"---\")) == 1:\n",
    "    text = r.data.split(\"---\")[0]\n",
    "else:\n",
    "    text = r.data.split(\"---\")[1]\n",
    "print(text)\n",
    "\n",
    "## Setting extraction agent to be gpt-4o\n",
    "## Extracted data will be formatted according to HawaiianPlant schema\n",
    "openai_extraction_agent = Agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    result_type=HawaiianPlant,\n",
    "    system_prompt = system_prompt,\n",
    ")\n",
    "\n",
    "## Data to be extracted from input image\n",
    "image_urls = [\n",
    "    f\"data:image/png;base64,{base64_image}\",\n",
    "]\n",
    "\n",
    "## Setting chat parameters: low detail (for efficiency) from input image\n",
    "image_params = [\n",
    "    ChatCompletionContentPartImageParam(\n",
    "        type='image_url', \n",
    "        image_url=ImageURL(url=url, detail='low')\n",
    "    ) for url in image_urls\n",
    "]\n",
    "\n",
    "## Setting chat prompt to 'user_prompt'\n",
    "msg_open_ai = [\n",
    "            ChatCompletionContentPartTextParam(text=user_prompt, type='text'),\n",
    "            *image_params\n",
    "]\n",
    "\n",
    "## Running data extraction\n",
    "r = await openai_extraction_agent.run(msg_open_ai)\n",
    "\n",
    "## Setting results to 'gpt_4o_output'\n",
    "gpt_4o_output = r.data\n",
    "\n",
    "print('Done: gpt_4o_output')\n",
    "\n",
    "## Setting extraction agent to be Claude Sonnet model\n",
    "## Extracted data will be formatted according to HawaiianPlant schema\n",
    "sonnet_extraction_agent = Agent(\n",
    "    model=\"anthropic:claude-3-5-sonnet-latest\",\n",
    "    result_type=HawaiianPlant,\n",
    "    system_prompt = system_prompt,\n",
    "    model_settings = {'temperature': 0.2}\n",
    "\n",
    ")\n",
    "\n",
    "## Setting context\n",
    "msg_claude = [\n",
    "    ChatCompletionContentPartTextParam(text=user_prompt, type='text'),\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": f\"{base64_image}\",\n",
    "                    }\n",
    "                },\n",
    "    \n",
    "]\n",
    "\n",
    "## Extracting data from image\n",
    "r = await sonnet_extraction_agent.run(msg_claude)\n",
    "sonnet_output = r.data\n",
    "\n",
    "print('Done: sonnet_output')\n",
    "\n",
    "\n",
    "## Setting system prompt\n",
    "gptmini_extraction_agent = Agent(\n",
    "    model=\"openai:o3-mini\",\n",
    "    result_type=HawaiianPlant,\n",
    "    system_prompt = system_prompt,\n",
    ")\n",
    "\n",
    "## Extracting output from extracted text\n",
    "r = await gptmini_extraction_agent.run(text)\n",
    "\n",
    "## Setting output\n",
    "gpt_mini_output = r.data\n",
    "\n",
    "print('Done: miniGPT ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"/Users/williamharrigan/Desktop/Github/ai_wagner_trait_data_extraction/files/man_extract.csv\")\n",
    "df.columns = df.columns.str.lower()\n",
    "df.head()\n",
    "type_cols = [col for col in df.columns if '_type_' in col]\n",
    "# type_cols\n",
    "# Group columns by their prefix (everything before '_type_' + last element)\n",
    "col_groups = {}\n",
    "for col in type_cols:\n",
    "    prefix = '_'.join(col.split('_')[:-1])\n",
    "    # print(prefix)\n",
    "    if prefix not in col_groups:\n",
    "        col_groups[prefix] = []\n",
    "    col_groups[prefix].append(col)\n",
    "    # print(col)\n",
    "\n",
    "# # Create new collapsed columns\n",
    "for prefix, cols in col_groups.items():\n",
    "    df[prefix] = df.apply(lambda row: [col.split('_')[-1].upper() for col in cols if row[col] == 1], axis=1)\n",
    "    \n",
    "# Drop the original type columns\n",
    "df.drop(columns=type_cols, inplace=True)\n",
    "\n",
    "df['hawaiian_name'] = df[['hawaiian_name_1', 'hawaiian_name_2', 'hawaiian_name_3', 'hawaiian_name_4']].apply(\n",
    "    lambda row: {x for x in row if pd.notna(x)}, axis=1\n",
    ")\n",
    "\n",
    "# Drop original columns if needed\n",
    "df.drop(columns=['hawaiian_name_1', 'hawaiian_name_2', 'hawaiian_name_3', 'hawaiian_name_4'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# hawaiian_cols = [col for col in df.columns if 'hawaiian_name' in col]\n",
    "# # Merge values into a list, ensuring all values are strings and filtering out empty values\n",
    "# df['hawaiian_name'] = df[hawaiian_cols].apply(lambda row: [str(val) for val in row if pd.notna(val) and str(val).strip() != ''], axis=1)\n",
    "\n",
    "# # Drop the original columns\n",
    "# df.drop(columns=hawaiian_cols, inplace=True)\n",
    "new_df = collapse_measurements(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = new_df[(new_df['family'] == family) & \n",
    "                      (new_df['genus'] == genus) & \n",
    "                      (new_df['species'] == species)].to_dict(orient='records')[0]\n",
    "\n",
    "model_outputs = {}\n",
    "model_outputs['gpt_4o_output'] = dict(gpt_4o_output)\n",
    "model_outputs['gpt_mini_output'] = dict(gpt_mini_output)\n",
    "model_outputs['sonnet_output'] = dict(sonnet_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth family: Araliaceae\n",
      "Model gpt_4o_output: Araliaceae\n",
      "Model gpt_mini_output: Araliaceae\n",
      "Model sonnet_output: Araliaceae\n",
      "\n",
      "Groundtruth genus: Cheirodendron\n",
      "Model gpt_4o_output: Cheirodendron\n",
      "Model gpt_mini_output: Cheirodendron\n",
      "Model sonnet_output: Cheirodendron\n",
      "\n",
      "Groundtruth species: forbesii\n",
      "Model gpt_4o_output: forbesii\n",
      "Model gpt_mini_output: forbesii\n",
      "Model sonnet_output: forbesii\n",
      "\n",
      "Groundtruth common_name: olapa, lapalapa\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth wagner_pg_number: pg 225,227\n",
      "Model gpt_4o_output: 227\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: 227\n",
      "\n",
      "Groundtruth description: Dicots\n",
      "Model gpt_4o_output: Dicots\n",
      "Model gpt_mini_output: Dicots\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth infraspecific_epithet: nan\n",
      "Model gpt_4o_output: subsp.\n",
      "Model gpt_mini_output: \n",
      "Model sonnet_output: forbesii\n",
      "\n",
      "Groundtruth stem_hair_type: G\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth phyllotaxy_type: O\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaf_hair_description: glabrous\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaf_hair_upper_description: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaf_hair_lower_description: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth breeding_type: A\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth inflorescence_type: U\n",
      "Model gpt_4o_output: UMBEL\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth ray_color: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth floret_color: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth spathe_color: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_outer_color: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_inner_color: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_color: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth labellum_color: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth corolla_type: Tri to O to Ob\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_corolla_type: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_corolla_type: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth corolla_color: Purple externally, greenish within\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth fruit_type: D\n",
      "Model gpt_4o_output: DRUPE\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: BERRY\n",
      "\n",
      "Groundtruth ploidy: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth chromosome_number: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth average_chromosome_number: nan\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth origin: END\n",
      "Model gpt_4o_output: ENDEMIC\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth fed_status: NS\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth status: Secure\n",
      "Model gpt_4o_output: RARE\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: ENDEMIC\n",
      "\n",
      "Groundtruth life_form_type: ['T', 'ST']\n",
      "Model gpt_4o_output: TREE\n",
      "Model gpt_mini_output: TREE\n",
      "Model sonnet_output: TREE\n",
      "\n",
      "Groundtruth leaf_type: ['COMPOUND']\n",
      "Model gpt_4o_output: COMPOUND\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: COMPOUND\n",
      "\n",
      "Groundtruth leaf_margin_type: []\n",
      "Model gpt_4o_output: ENTIRE\n",
      "Model gpt_mini_output: ENTIRE\n",
      "Model sonnet_output: ENTIRE\n",
      "\n",
      "Groundtruth leaf_shape_type: []\n",
      "Model gpt_4o_output: OBOVATE\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: ELLIPTIC\n",
      "\n",
      "Groundtruth juvenile_leaf_type: []\n",
      "Model gpt_4o_output: SIMPLE\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: COMPOUND\n",
      "\n",
      "Groundtruth juvenile_leaf_margin_type: []\n",
      "Model gpt_4o_output: ENTIRE\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth juvenile_leaf_shape_type: []\n",
      "Model gpt_4o_output: OVATE\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaflets_leaf_type: ['SIMPLE']\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaflets_leaf_margin_type: ['NOTEETH']\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaflets_leaf_shape_type: ['E', 'OVA']\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaf_hair_type: ['G']\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaf_hair_upper_type: []\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaf_hair_lower_type: []\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth juvenile_leaf_hair_type: []\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth island_type: ['KAU']\n",
      "Model gpt_4o_output: [<Location.MOLOKAI: 'MOLOKAI'>]\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: [<Location.KAUAI: 'KAUAI'>]\n",
      "\n",
      "Groundtruth hawaiian_name: {'lapalapa', '`Olapa'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth stem_height: {'exmin': nan, 'min': 3.0, 'max': 4.5, 'exmax': nan, 'unit': 'm'}\n",
      "Model gpt_4o_output: min=None max=4.0 extreme_min=None extreme_max=None\n",
      "Model gpt_mini_output: min=3.0 max=4.0 extreme_min=None extreme_max=None\n",
      "Model sonnet_output: min=3.0 max=4.5 extreme_min=None extreme_max=None\n",
      "\n",
      "Groundtruth leaf_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: min=None max=35.0 extreme_min=None extreme_max=None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: min=50.0 max=120.0 extreme_min=None extreme_max=None\n",
      "\n",
      "Groundtruth leaf_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: min=None max=15.0 extreme_min=None extreme_max=None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth juvenile_leaf_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: min=None max=35.0 extreme_min=None extreme_max=None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth juvenile_leaf_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: min=None max=15.0 extreme_min=None extreme_max=None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaflets_leaf_length: {'exmin': nan, 'min': 5.0, 'max': 12.0, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth leaflets_leaf_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth petioles: {'exmin': nan, 'min': 3.0, 'max': 7.0, 'exmax': 10.0, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: min=0.3 max=1.0 extreme_min=None extreme_max=None\n",
      "Model sonnet_output: min=3.0 max=7.0 extreme_min=None extreme_max=None\n",
      "\n",
      "Groundtruth staminate_inflorescence_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_inflorescence_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_inflorescence_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_inflorescence_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth inflorescence_flower_length: {'exmin': nan, 'min': nan, 'max': 100.0, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth inflorescence_flower_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth flower_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth flower_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth rachis_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth rachis_diameter: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth head_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth head_diameter: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth bur_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth tepal_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_tepal_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_tepal_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth ray_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth ray_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth florets_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth involucre_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth involucre_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_involucre_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistilate_involucre_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth bract_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth bract_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth bract_lower_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth bract_outer_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth bracteoles_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth bracteoles_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pedicel_length: {'exmin': nan, 'min': 3.0, 'max': 6.0, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: min=3.0 max=6.0 extreme_min=None extreme_max=None\n",
      "\n",
      "Groundtruth pedicel_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_pedicel_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_pedicel_width: {'exmin': nan, 'min': None, 'max': None, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_pedicel_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth hypanthium_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth hypanthium_width: {'exmin': nan, 'min': None, 'max': None, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth peduncle_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth peduncle_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_peduncle_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_peduncle_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_peduncle_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_peduncle_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth spathe_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth spathe_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth spadix_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_outer_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_outer_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_inner_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_inner_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_tube_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_lobes_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth perianth_lobes_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_perianth_tube_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_perianth_tube_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pappus_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth umbellet_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth labellum_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth labellum_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth calyx_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth calyx_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth calyx_teeth_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth calyx_teeth_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth calyx_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth calyx_lobes_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth upper_calyx_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth lower_calyx_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth inner_calyx_lobes_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth inner_calyx_lobes_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth outer_calyx_lobes_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth outer_calyx_lobes_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth calyx_tube_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth calyx_tube_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_calyx_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_calyx_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_calyx_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_calyx_lobes_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_calyx_length: {'exmin': nan, 'min': female_calyx_length_mm_min    NaN\n",
      "female_calyx_length_mm_min    NaN\n",
      "Name: 14, dtype: object, 'max': female_calyx_length_mm_max    NaN\n",
      "female_calyx_length_mm_max    NaN\n",
      "Name: 14, dtype: object, 'exmax': female_calyx_length_mm_exmax    NaN\n",
      "female_calyx_length_mm_exmax    NaN\n",
      "Name: 14, dtype: object, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_calyx_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_calyx_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_calyx_lobes_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_calyx_lobes_length_inner: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_calyx_lobes_length_outer: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_calyx_lobes_width_outer: {'exmin': None, 'min': male_calyx_lobes_width_outer_mm_min    NaN\n",
      "male_calyx_lobes_width_outer_mm_min    NaN\n",
      "Name: 14, dtype: object, 'max': male_calyx_lobes_width_outer_mm_max    NaN\n",
      "male_calyx_lobes_width_outer_mm_max    NaN\n",
      "Name: 14, dtype: object, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_calyx_tube_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_calyx_lobes_length_inner: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_calyx_lobes_length_outer: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_calyx_lobes_width_inner: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_calyx_lobes_width_outer: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth inner_calyx_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth outer_calyx_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth corolla_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth corolla_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth corolla_tube_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth corolla_tube_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth corolla_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth corolla_lobes_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth upper_corolla: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth lower_corolla: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth upper_corolla_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth lower_corolla_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth corolla_lip: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_corolla_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_corolla_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_corolla_tube_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth staminate_corolla_tube_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_corolla_tube_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth pistillate_corolla_tube_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_corolla_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth female_corolla_lobes_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_corrola_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth male_corrola_lobes_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth fruit_length: {'exmin': nan, 'min': 4.0, 'max': 5.0, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: min=4.0 max=5.0 extreme_min=None extreme_max=None\n",
      "\n",
      "Groundtruth fruit_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: min=4.0 max=5.0 extreme_min=None extreme_max=None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth fruit_diameter: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: min=4.0 max=5.0 extreme_min=None extreme_max=None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth seeds_perfruit: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'count'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth seed_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth seed_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n",
      "Groundtruth seed_diameter: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None\n",
      "Model gpt_mini_output: None\n",
      "Model sonnet_output: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for gt_key, gt_value in ground_truth.items():\n",
    "    print(f\"Groundtruth {gt_key}: {gt_value}\")\n",
    "    for model in model_outputs.keys():\n",
    "        \n",
    "        print(f\"Model {model}: {model_outputs[model][gt_key]}\")\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for model, results in model_outputs.items():\n",
    "    print(model)\n",
    "    print(results['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AreAnntoationsEqual(BaseModel):\n",
    "    are_equal: bool = Field(..., description=\"Are the two annotations equal\")\n",
    "    justification: str = Field(..., description=\"Justification of the propose value for are_equal\")\n",
    "    \n",
    "\n",
    "## Setting the prompt and model for validation agent\n",
    "validation_agent = Agent(\n",
    "    model=\"groq:llama-3.3-70b-specdec\",\n",
    "    result_type=AreAnntoationsEqual,\n",
    "    system_prompt = \"\"\"You are an expert taxonomist. You are comparing the outcome of a manually extracted result versus an automatically extracted result. You need to compare the automatic results and determine whether the result is synonymous or equal the manual one; taking into consideration\n",
    "    linguisitc and formatting nuances. Your answer is whether the two results are similar True/False and a justificaiton for your answer\"\"\",\n",
    ")\n",
    "\n",
    "## Models to compare\n",
    "# models = [\"gpt_4o_output\", \"gpt_mini_output\", \"sonnet_output\", \"deepseek_llama\"]\n",
    "models = [\"gpt_4o_output\", \"gpt_mini_output\", \"sonnet_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for gt_key, gt_value in ground_truth.items():\n",
    "#     print(f\"Groundtruth {gt_key}: {gt_value}\")\n",
    "#     for model in model_outputs.keys():\n",
    "#         # user_prompt = f\"\"\" manually annotated {gt_key}: {gt_value}\\n automatically annotated {gt_key}:  {model_outputs[model][gt_key],  gt_key}\n",
    "#         # \"\"\"\n",
    "#         user_prompt = f\"\"\" manually annotated {gt_key}: {gt_value}\\n automatically annotated {gt_key}:  {model_outputs[model][gt_key],  gt_key} \"\"\"\n",
    "\n",
    "#         # print(\"\\n\\n\"+user_prompt)\n",
    "#         result = await validation_agent.run(user_prompt)\n",
    "#         # print(result.data)\n",
    "#         print(f\"Model {model}: {model_outputs[model][gt_key]} \\t Accurate output: {result.data.are_equal}\")\n",
    "#         print()\n",
    "#         # print(f\"{prop}:{getattr(eval(model),  prop)}:{r.data.are_equal}\", end=\"\\t\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtruth family: Araliaceae\n",
      "Model gpt_4o_output: Araliaceae \t Accurate output: True\n",
      "Justification: The manually annotated family 'Araliaceae' and the automatically annotated family 'Araliaceae' are identical, with no linguistic or formatting nuances to suggest otherwise.\n",
      "Model gpt_mini_output: Araliaceae \t Accurate output: True\n",
      "Justification: The manually annotated family 'Araliaceae' is exactly the same as the automatically annotated family 'Araliaceae', indicating a perfect match with no linguistic or formatting nuances to consider.\n",
      "Model sonnet_output: Araliaceae \t Accurate output: True\n",
      "Justification: The manually annotated family 'Araliaceae' and the automatically annotated family 'Araliaceae' are identical, with no linguistic or formatting nuances that would suggest they are not equal.\n",
      "\n",
      "Groundtruth genus: Cheirodendron\n",
      "Model gpt_4o_output: Cheirodendron \t Accurate output: True\n",
      "Justification: The manually annotated genus 'Cheirodendron' exactly matches the automatically annotated genus 'Cheirodendron', indicating that they are equal.\n",
      "Model gpt_mini_output: Cheirodendron \t Accurate output: True\n",
      "Justification: The manually annotated genus and the automatically annotated genus are identical, with both being 'Cheirodendron'. This suggests that the automatic extraction accurately matched the manual annotation, resulting in equal results.\n",
      "Model sonnet_output: Cheirodendron \t Accurate output: True\n",
      "Justification: The manually annotated genus 'Cheirodendron' is identical to the automatically annotated genus 'Cheirodendron', as they are character-for-character the same, indicating no linguistic or formatting differences.\n",
      "\n",
      "Groundtruth species: forbesii\n",
      "Model gpt_4o_output: forbesii \t Accurate output: True\n",
      "Justification: The manually annotated species 'forbesii' and the automatically annotated species 'forbesii' are identical, with no linguistic or formatting nuances that would suggest otherwise.\n",
      "Model gpt_mini_output: forbesii \t Accurate output: True\n",
      "Justification: The manually annotated species 'forbesii' and the automatically annotated species 'forbesii' are identical, with no linguistic or formatting nuances that would suggest they are not equal.\n",
      "Model sonnet_output: forbesii \t Accurate output: True\n",
      "Justification: The manually annotated species 'forbesii' is identical to the automatically annotated species 'forbesii', indicating a perfect match with no linguistic or formatting nuances to consider.\n",
      "\n",
      "Groundtruth common_name: olapa, lapalapa\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated common name is 'olapa' or 'lapalapa', but the automatically annotated common name is 'None', indicating that the automatic annotation was unable to identify a common name, therefore the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated common name is 'olapa' or 'lapalapa', but the automatically annotated common name is 'None', indicating a lack of a match. This discrepancy suggests that the automatic annotation process failed to identify a common name, while the manual annotation provided specific names.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated common name is 'olapa' or 'lapalapa', while the automatically annotated common name is 'None', indicating that the automatic annotation was unable to identify a common name, resulting in a lack of equivalence between the two annotations.\n",
      "\n",
      "Groundtruth wagner_pg_number: pg 225,227\n",
      "Model gpt_4o_output: 227 \t Accurate output: False\n",
      "Justification: The manually annotated result includes two page numbers (pg 225,227), while the automatically annotated result only includes one page number (227), indicating that they are not equal due to the difference in the number of pages referenced.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'pg 225,227' while the automatically annotated result is 'None', indicating that the automatic extraction failed to provide a value, and therefore the two results are not equal.\n",
      "Model sonnet_output: 227 \t Accurate output: False\n",
      "Justification: The manually annotated result includes two page numbers (225 and 227), while the automatically annotated result only includes one page number (227), indicating that they are not equal.\n",
      "\n",
      "Groundtruth description: Dicots\n",
      "Model gpt_4o_output: Dicots \t Accurate output: True\n",
      "Justification: The manually annotated description 'Dicots' and the automatically annotated description 'Dicots' are identical, indicating that they are equal.\n",
      "Model gpt_mini_output: Dicots \t Accurate output: True\n",
      "Justification: The manually annotated description 'Dicots' and the automatically annotated description 'Dicots' are identical, indicating that they are equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated description is 'Dicots', while the automatically annotated description is 'None', indicating that the automatic extraction failed to provide a result, and therefore the two annotations are not equal.\n",
      "\n",
      "Groundtruth infraspecific_epithet: nan\n",
      "Model gpt_4o_output: subsp. \t Accurate output: False\n",
      "Justification: The manually annotated infraspecific_epithet is 'nan', which is an abbreviation for 'not a number' or 'not applicable', while the automatically annotated infraspecific_epithet is 'subsp.', which is an abbreviation for 'subspecies'. These two annotations are not equal due to the difference in their meaning and formatting.\n",
      "Model gpt_mini_output:  \t Accurate output: True\n",
      "Justification: Both the manually annotated and automatically annotated infraspecific_epithet are empty, indicating a lack of information or no subspecies designation, thus they can be considered equal.\n",
      "Model sonnet_output: forbesii \t Accurate output: False\n",
      "Justification: The manually annotated infraspecific_epithet is 'nan', indicating that it is unknown or not applicable, whereas the automatically annotated infraspecific_epithet is 'forbesii', which is a specific epithet. These two values are not equal, as 'nan' does not provide any meaningful information, while 'forbesii' is a distinct and valid epithet.\n",
      "\n",
      "Groundtruth stem_hair_type: G\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated stem_hair_type is 'G', but the automatically annotated stem_hair_type is 'None', indicating that they are not equal due to the absence of a value in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manual annotation has a value of 'G' for stem_hair_type, while the automatic annotation has a value of 'None', indicating that they are not equal due to the presence of a value in the manual annotation and the absence of a value in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'G' while the automatically annotated result is 'None', indicating that they are not equal.\n",
      "\n",
      "Groundtruth phyllotaxy_type: O\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manual annotation has a value of 'O' for phyllotaxy_type, while the automatic annotation has a value of 'None', indicating that they are not equal due to the presence of a value in the manual annotation and the absence of a value in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'O' while the automatically annotated result is 'None', indicating they are not equal due to the presence of a value in the manual annotation and the absence of a value in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'O' while the automatically annotated result is 'None', indicating they are not equal due to the presence of a value in the manual annotation and the absence of a value in the automatic annotation.\n",
      "\n",
      "Groundtruth leaf_hair_description: glabrous\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'glabrous', which means the leaf is smooth or hairless, while the automatically annotated result is 'None', indicating no information or a missing value. These two results are not equal, as one provides a specific description and the other does not.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result 'glabrous' indicates that the leaf has no hairs, while the automatically annotated result 'None' is unclear and does not provide a direct equivalent description, suggesting that the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'glabrous', indicating that the leaf is hairless, while the automatically annotated result is 'None', indicating that no information about leaf hair is available. These results are not equal because 'glabrous' provides specific information about the presence or absence of hair, while 'None' does not provide any information.\n",
      "\n",
      "Groundtruth leaf_hair_upper_description: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for leaf_hair_upper_description are empty or null (nan and None respectively), indicating a lack of information or no presence of upper leaf hair, thus they can be considered equal.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent the absence of a value, making them equivalent in this context.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated and automatically annotated results for leaf_hair_upper_description are equivalent to 'no value' or 'empty', with 'nan' representing Not A Number in manual annotation and 'None' representing the absence of a value in automatic annotation.\n",
      "\n",
      "Groundtruth leaf_hair_lower_description: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both the manually and automatically annotated values for leaf_hair_lower_description are nan and None respectively, which essentially represent the same concept of 'no data' or 'missing value' in different formats.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: The manually annotated value 'nan' and the automatically annotated value 'None' are equivalent in meaning, as 'nan' typically stands for 'not a number' and 'None' represents the absence of a value, both indicating that there is no information or data available for the leaf hair lower description.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for leaf_hair_lower_description are empty or null (nan and None respectively), indicating a lack of information or no hair on the lower leaf, thus they can be considered equal.\n",
      "\n",
      "Groundtruth breeding_type: A\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated breeding_type is 'A', while the automatically annotated breeding_type is 'None', indicating that they are not equal due to the presence of a value in the manual annotation and the absence of a value in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated breeding_type is 'A', but the automatically annotated breeding_type is 'None', indicating that the two annotations are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated breeding_type is 'A' while the automatically annotated breeding_type is 'None', indicating that they are not equal due to the absence of a value in the automatic annotation.\n",
      "\n",
      "Groundtruth inflorescence_type: U\n",
      "Model gpt_4o_output: UMBEL \t Accurate output: False\n",
      "Justification: The manually annotated inflorescence_type is 'U' which is possibly an abbreviation for a term, while the automatically annotated inflorescence_type is 'UMBEL', which is a specific type of inflorescence. Given that 'U' could potentially represent 'UMBEL' if 'U' is an abbreviation for 'UMBEL', however, without explicit confirmation that 'U' stands for 'UMBEL', it cannot be concluded that they are equal. Hence, the annotations are not equal due to the lack of clarity and explicit matching between 'U' and 'UMBEL'.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated inflorescence_type is 'U', while the automatically annotated inflorescence_type is 'None', indicating that they are not equal due to the absence of a value in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated inflorescence_type is 'U', while the automatically annotated inflorescence_type is 'None', indicating that they are not equal due to the absence of a value in the automatic annotation.\n",
      "\n",
      "Groundtruth ray_color: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent undefined or missing values in different programming contexts. In this case, they are equivalent in meaning, with 'nan' typically used in numerical computations and 'None' used for general null or undefined values in Python.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent the absence of a value, with 'nan' typically used in numerical contexts and 'None' in more general cases. Given the context of 'ray_color', it is reasonable to consider them equivalent in this scenario.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent undefined or null values in their respective contexts, with 'nan' typically used in numerical computations and 'None' in programming. Despite the difference in representation, they convey the same meaning of an absence of value.\n",
      "\n",
      "Groundtruth floret_color: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent unknown or undefined values, making them equivalent in this context.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: The manually annotated result 'nan' and the automatically annotated result 'None' both represent a null or missing value, indicating that the floret color is unknown or not provided. Although they are represented differently, they convey the same meaning, thus they are considered equal.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent an undefined or missing value, making them synonymous in this context.\n",
      "\n",
      "Groundtruth spathe_color: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' are used to represent missing or undefined values, and in this context, they can be considered equivalent, with 'nan' often being used in numerical contexts and 'None' in more general cases. Thus, the manual and automatic annotations of spathe_color are equal.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for spathe_color are essentially null or unknown, with 'nan' and 'None' being equivalent representations of missing or undefined values in different formatting conventions.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' are used to represent missing or undefined values, making them synonymous in this context.\n",
      "\n",
      "Groundtruth perianth_outer_color: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent undefined or missing values, making them synonymous in this context.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for perianth_outer_color are essentially null or missing, with 'nan' being a common representation of missing numeric data and 'None' being a common representation of missing data in general, thus they can be considered equal in this context.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated and automatically annotated values for perianth_outer_color are essentially null or missing, with 'nan' and 'None' being common representations of null or missing values in different contexts.\n",
      "\n",
      "Groundtruth perianth_inner_color: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: The manually annotated value 'nan' and the automatically annotated value 'None' are equivalent, as 'nan' is often used to represent missing or undefined values in manual annotations, while 'None' serves the same purpose in automatic annotations.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for perianth_inner_color are essentially null or undefined, with 'nan' and 'None' being common representations of missing or undefined values in different contexts. Therefore, they can be considered equal in this case.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for perianth_inner_color are essentially null or missing values, with 'nan' and 'None' being common representations of missing data in different contexts. Therefore, they can be considered equal in this context.\n",
      "\n",
      "Groundtruth perianth_color: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent null or missing values, indicating that the perianth color was not specified or could not be determined in either the manual or automatic annotations.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent the absence of a value, making them equivalent in this context.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent null or missing values, indicating that the perianth color is unknown or not provided in both the manual and automatic annotations.\n",
      "\n",
      "Groundtruth labellum_color: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: The manually annotated 'nan' and automatically annotated 'None' are equivalent in meaning, as both represent the absence of a value for labellum_color.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated and automatically annotated results for labellum_color are equivalent to no value or unknown, with 'nan' and 'None' being common representations of null or undefined values in different contexts.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' are used to represent the absence of a value, with 'nan' typically used in numerical contexts and 'None' in non-numerical contexts. Given the context of 'labellum_color', both annotations imply that the color is unknown or not applicable, making them synonymous.\n",
      "\n",
      "Groundtruth corolla_type: Tri to O to Ob\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manual annotation for corolla_type is 'Tri to O to Ob', while the automatic annotation is 'None', indicating that the automatic annotation did not provide a value, whereas the manual annotation did, therefore they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated corolla_type is 'Tri to O to Ob', while the automatically annotated corolla_type is 'None', indicating a clear discrepancy between the two results.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated corolla_type is 'Tri to O to Ob', while the automatically annotated corolla_type is 'None', indicating a clear discrepancy between the two results.\n",
      "\n",
      "Groundtruth staminate_corolla_type: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both manually and automatically annotated values for staminate_corolla_type are null or empty (nan and None respectively), indicating the absence of a value, thus they can be considered equal.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for staminate_corolla_type are equivalent to null or empty values, with 'nan' representing 'not a number' or null in manual annotation, and 'None' explicitly meaning null in automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for staminate_corolla_type are essentially indicating a null or missing value, with 'nan' and 'None' being common representations of this concept in different contexts.\n",
      "\n",
      "Groundtruth pistillate_corolla_type: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for pistillate_corolla_type are equivalent to null or undefined, with 'nan' representing 'not a number' and 'None' representing the absence of a value, thus they can be considered equal in this context.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated and automatically annotated values for pistillate_corolla_type are essentially null or empty, with 'nan' and 'None' being common representations of missing or undefined values in different contexts.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: The manually annotated value 'nan' and the automatically annotated value 'None' are equivalent in meaning, as both represent an unknown or missing value.\n",
      "\n",
      "Groundtruth corolla_color: Purple externally, greenish within\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manual annotation provides a detailed description of the corolla color, stating it is purple externally and greenish within, while the automatic annotation simply states 'None', indicating a lack of information or a failure to extract the color. This significant difference in content and detail between the two annotations means they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result provides a specific description of the corolla color, stating it is purple externally and greenish within, while the automatically annotated result simply states 'None', indicating a complete lack of information. This discrepancy suggests that the two results are not equal, as one provides detailed information and the other provides no information.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result describes the corolla color as 'Purple externally, greenish within', while the automatically annotated result states 'None', indicating a lack of information. These two results are not equal due to the absence of any color description in the automatic annotation.\n",
      "\n",
      "Groundtruth fruit_type: D\n",
      "Model gpt_4o_output: DRUPE \t Accurate output: True\n",
      "Justification: Although the manually annotated fruit type is 'D' and the automatically annotated fruit type is 'DRUPE', in botanical terms, 'D' can be an abbreviation for 'DRUPE', which refers to a specific type of fruit that has a hard stone or pit in the center, such as a peach or plum. Therefore, considering the linguistic and formatting nuances, the two results can be considered equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'D' while the automatically annotated result is 'None', indicating that they are not equal due to the absence of a value in the automatic annotation.\n",
      "Model sonnet_output: BERRY \t Accurate output: False\n",
      "Justification: The manually annotated fruit_type is 'D' which does not match the automatically annotated fruit_type 'BERRY', indicating they are not equal due to differences in classification and formatting.\n",
      "\n",
      "Groundtruth ploidy: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both 'nan' and 'None' represent the absence of a value, which makes them equivalent in this context.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated ploidy and the automatically annotated ploidy are essentially null or undefined values, with 'nan' representing 'not a number' and 'None' indicating the absence of a value. Therefore, they can be considered equal in this context.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated ploidy and the automatically annotated ploidy are equivalent to null or undefined values, with 'nan' representing Not A Number and 'None' representing the absence of a value. Therefore, they can be considered equal in this context.\n",
      "\n",
      "Groundtruth chromosome_number: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated and automatically annotated chromosome_number are null or undefined, with 'nan' and 'None' being equivalent representations of null or undefined values in different programming contexts.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated and automatically annotated results for chromosome_number are empty or null (nan and None respectively), indicating a lack of information or absence of a value, which can be considered equivalent in this context.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: The manually annotated result 'nan' and the automatically annotated result 'None' both represent unknown or missing values, making them synonymous in this context.\n",
      "\n",
      "Groundtruth average_chromosome_number: nan\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both manual and automatic annotations for average_chromosome_number are empty or null (nan and None respectively), indicating that no value was found or provided for this field. In this context, they can be considered equal as they convey the same information - the absence of a specific value.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: The manually annotated value 'nan' and the automatically annotated value 'None' both represent the absence of a value, making them equal in this context.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for average_chromosome_number are essentially indicating a lack of a value, with 'nan' meaning 'not a number' and 'None' signifying the absence of a value. These two terms are often used interchangeably in different contexts to denote missing or undefined data, thus the annotations can be considered equal.\n",
      "\n",
      "Groundtruth origin: END\n",
      "Model gpt_4o_output: ENDEMIC \t Accurate output: False\n",
      "Justification: The manually annotated origin 'END' and the automatically annotated origin 'ENDEMIC' are not equal due to differences in spelling and meaning. 'END' is a common abbreviation, while 'ENDEMIC' is a term used in biology to describe a species that is native to a specific region.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated origin is 'END' while the automatically annotated origin is 'None', indicating that they are not equal due to the presence of a value in the manual annotation and the absence of a value in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated origin is 'END' while the automatically annotated origin is 'None', indicating a clear discrepancy between the two annotations.\n",
      "\n",
      "Groundtruth fed_status: NS\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result 'NS' does not match the automatically annotated result 'None', indicating a possible discrepancy in the annotation process.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manual annotation 'NS' and the automatic annotation 'None' are not equal, as 'NS' typically stands for 'Not Specified' or 'No Status', whereas 'None' implies a complete absence of information. Although both may seem similar, they have different implications in the context of federal status.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manual annotation 'NS' and the automatic annotation 'None' are not equal, as 'NS' is an abbreviation that may stand for 'Not Specified' or a similar term, whereas 'None' explicitly indicates the absence of a value. The two annotations may be related in meaning, but they are not identical.\n",
      "\n",
      "Groundtruth status: Secure\n",
      "Model gpt_4o_output: RARE \t Accurate output: False\n",
      "Justification: The manually annotated status is 'Secure' while the automatically annotated status is 'RARE'. These two terms are not synonymous, as 'Secure' implies a sense of safety or protection, while 'RARE' implies something is uncommon or infrequently found. Therefore, the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated status is 'Secure' while the automatically annotated status is 'None', indicating that the two results are not equal.\n",
      "Model sonnet_output: ENDEMIC \t Accurate output: False\n",
      "Justification: The manually annotated status 'Secure' and the automatically annotated status 'ENDEMIC' are not synonymous. 'Secure' typically refers to a state of being safe or protected, whereas 'ENDEMIC' refers to a species or disease that is native or unique to a specific geographic region. These terms have distinct meanings and are not interchangeable.\n",
      "\n",
      "Groundtruth life_form_type: ['T', 'ST']\n",
      "Model gpt_4o_output: TREE \t Accurate output: False\n",
      "Justification: The manually annotated life_form_type is an array of abbreviations ['T', 'ST'], while the automatically annotated life_form_type is 'TREE', which is an expanded form of 'T' but does not match 'ST'. Therefore, the two results are not equal.\n",
      "Model gpt_mini_output: TREE \t Accurate output: False\n",
      "Justification: The manually annotated life_form_type is a list containing 'T' and 'ST', while the automatically annotated life_form_type is 'TREE'. Although 'T' and 'TREE' might be related, they are not exactly the same, and 'ST' does not directly correspond to 'TREE', indicating that the two annotations are not equal due to differences in specification and possibly categorization.\n",
      "Model sonnet_output: TREE \t Accurate output: False\n",
      "Justification: The manually annotated life_form_type is ['T', 'ST'], which appears to be abbreviated forms, possibly standing for 'Tree' and 'Shrub/Tree'. In contrast, the automatically annotated life_form_type is 'TREE', which is a more specific classification. While 'TREE' could be considered a subset of the manually annotated types, it does not fully match the range of possibilities represented by ['T', 'ST']. Therefore, the two annotations are not equal due to differences in specificity and potential inclusivity of types.\n",
      "\n",
      "Groundtruth leaf_type: ['COMPOUND']\n",
      "Model gpt_4o_output: COMPOUND \t Accurate output: True\n",
      "Justification: The manually annotated result 'COMPOUND' and the automatically annotated result 'COMPOUND' are the same, despite the manual result being in a list format and the automatic result being a single value. The list format in the manual result is likely due to the possibility of multiple leaf types being annotated, but in this case, it only contains one value, which matches the automatic result.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manual annotation has a value of 'COMPOUND' for leaf_type, but the automatic annotation has a value of None, indicating that they do not match.\n",
      "Model sonnet_output: COMPOUND \t Accurate output: True\n",
      "Justification: The manually annotated value 'COMPOUND' enclosed in a list and the automatically annotated value 'COMPOUND' are linguistically and semantically equivalent, despite the formatting difference of being enclosed in a list for the manual annotation.\n",
      "\n",
      "Groundtruth leaf_margin_type: []\n",
      "Model gpt_4o_output: ENTIRE \t Accurate output: False\n",
      "Justification: The manually annotated result is an empty list, while the automatically annotated result is 'ENTIRE', indicating that the automatic result is not equal to the manual result due to the difference in their values.\n",
      "Model gpt_mini_output: ENTIRE \t Accurate output: False\n",
      "Justification: The manually annotated result is an empty list, while the automatically annotated result is 'ENTIRE', indicating a difference in the leaf margin type annotation.\n",
      "Model sonnet_output: ENTIRE \t Accurate output: False\n",
      "Justification: The manually annotated result is an empty list, while the automatically annotated result is 'ENTIRE', indicating that the two results are not equal. The manually annotated result implies that no information is available or the leaf margin type is unknown, whereas the automatic result provides a specific type, suggesting a discrepancy between the two annotations.\n",
      "\n",
      "Groundtruth leaf_shape_type: []\n",
      "Model gpt_4o_output: OBOVATE \t Accurate output: False\n",
      "Justification: The manually annotated result is an empty list, while the automatically annotated result is a specific type (OBOVATE), indicating that the two results are not equal due to the absence of any information in the manual annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated and automatically annotated leaf_shape_type are empty or null, indicating a lack of information, which can be considered equal in this context.\n",
      "Model sonnet_output: ELLIPTIC \t Accurate output: False\n",
      "Justification: The manually annotated result is an empty list, indicating no leaf shape type was specified, while the automatically annotated result is 'ELLIPITIC', indicating a specific leaf shape type. These results are not equal due to the absence of any information in the manual annotation.\n",
      "\n",
      "Groundtruth juvenile_leaf_type: []\n",
      "Model gpt_4o_output: SIMPLE \t Accurate output: False\n",
      "Justification: The manually annotated result for juvenile_leaf_type is an empty list, indicating that no specific type was identified, whereas the automatically annotated result is 'SIMPLE', which is a specific type, thus the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for juvenile_leaf_type are empty or null, indicating a lack of information or no juvenile leaf type specified, thus they can be considered equal.\n",
      "Model sonnet_output: COMPOUND \t Accurate output: False\n",
      "Justification: The manually annotated result is an empty list, indicating no specific type, while the automatically annotated result is 'COMPOUND', indicating a specific type of juvenile leaf. These two results are not equal due to the absence of any information in the manual annotation and the presence of a specific type in the automatic annotation.\n",
      "\n",
      "Groundtruth juvenile_leaf_margin_type: []\n",
      "Model gpt_4o_output: ENTIRE \t Accurate output: False\n",
      "Justification: The manually annotated result is an empty list, indicating that no value was provided or the field is missing, whereas the automatically annotated result is 'ENTIRE', which is a specific value for the juvenile leaf margin type. These two results are not equal due to the presence of a specific value in the automatic annotation and the absence of any value in the manual annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for juvenile_leaf_margin_type are essentially empty, with the manual annotation being an empty list [] and the automatic annotation being None, which in this context can be considered synonymous as both imply the absence of any specified value.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: The manually annotated result is an empty list '[]' and the automatically annotated result is 'None'. In the context of this task, both can be considered as indicating the absence of a juvenile leaf margin type, thus they can be considered equal.\n",
      "\n",
      "Groundtruth juvenile_leaf_shape_type: []\n",
      "Model gpt_4o_output: OVATE \t Accurate output: False\n",
      "Justification: The manually annotated result is empty, while the automatically annotated result is 'OVATE', indicating a difference between the two annotations.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manually annotated and automatically annotated values for juvenile_leaf_shape_type are empty or null, indicating a lack of information or no specific type assigned, thus they can be considered equal in the context of lacking a value.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for juvenile_leaf_shape_type are empty or null, indicating a lack of information or no specific type assigned, thus they can be considered equal.\n",
      "\n",
      "Groundtruth leaflets_leaf_type: ['SIMPLE']\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific value for leaflets_leaf_type, which is 'SIMPLE', while the automatically annotated result has a value of None, indicating a lack of information or a null value. These two values are not equal, as one provides a specific classification and the other does not.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result indicates that the leaflets_leaf_type is 'SIMPLE', while the automatically annotated result is 'None', indicating a lack of information or a null value. These two results are not equal, as 'SIMPLE' provides a specific classification, whereas 'None' does not provide any classification.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'SIMPLE', while the automatically annotated result is 'None', indicating that the two results are not equal due to the absence of a value in the automatic annotation.\n",
      "\n",
      "Groundtruth leaflets_leaf_margin_type: ['NOTEETH']\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a value of 'NOTEETH', while the automatically annotated result is 'None', indicating that they are not equal due to the absence of a value in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'NOTEETH', while the automatically annotated result is 'None', indicating that the two results are not equal due to the absence of a value in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'NOTEETH', while the automatically annotated result is 'None', indicating that the two results are not equal due to the absence of a value in the automatic annotation.\n",
      "\n",
      "Groundtruth leaflets_leaf_shape_type: ['E', 'OVA']\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manual annotation has specific values for leaflets_leaf_shape_type, namely 'E' and 'OVA', while the automatic annotation has a value of None, indicating a lack of information or a failure to annotate, thus the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated leaflets_leaf_shape_type is ['E', 'OVA'], while the automatically annotated leaflets_leaf_shape_type is None, indicating that the automatic extraction did not provide a value for this field, making them non-equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is ['E', 'OVA'] while the automatically annotated result is None, indicating that the two results are not equal due to the absence of a value in the automatic annotation.\n",
      "\n",
      "Groundtruth leaf_hair_type: ['G']\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'G' while the automatically annotated result is None, indicating that the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result 'G' does not match the automatically annotated result 'None', indicating that the automatic extraction failed to provide a value for the leaf_hair_type, while the manual annotation provided a specific value.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result for leaf_hair_type is 'G', while the automatically annotated result is None, indicating a absence of annotation. These two results are not equal due to the presence of a value in the manual annotation and the lack of a value in the automatic annotation.\n",
      "\n",
      "Groundtruth leaf_hair_upper_type: []\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for leaf_hair_upper_type are empty or null, indicating a lack of data or no hairs on the upper leaf surface. In this context, '[]' (empty list) and 'None' can be considered synonymous as they both convey the absence of a value.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: The manually annotated result '[]' and the automatically annotated result 'None' can be considered equal as both represent the absence of a value for 'leaf_hair_upper_type'.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: The manually annotated result is an empty list '[]' and the automatically annotated result is 'None'. In the context of this annotation, both can be considered as indicating the absence of any leaf hair upper type, thus they can be considered equal.\n",
      "\n",
      "Groundtruth leaf_hair_lower_type: []\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for leaf_hair_lower_type are effectively indicating the absence of information or no value, with the manual annotation being an empty list and the automatic annotation being None, which in this context can be interpreted as synonymous.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: The manual annotation is an empty list '[]' and the automatic annotation is 'None', both are semantically equivalent as they indicate the absence of a value.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: In this context, an empty list '[]' and 'None' can be considered equal as both imply the absence of any leaf hair lower type.\n",
      "\n",
      "Groundtruth juvenile_leaf_hair_type: []\n",
      "Model gpt_4o_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for juvenile_leaf_hair_type are essentially indicating the absence of information, with the manual annotation being an empty list and the automatic annotation being None, which in this context can be considered synonymous.\n",
      "Model gpt_mini_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for juvenile_leaf_hair_type are essentially indicating the absence of information or a null value, with '[]' representing an empty list in the manual annotation and 'None' representing a null value in the automatic annotation. These two can be considered synonymous in this context, as they both convey the same meaning of no data or no specific type being recorded.\n",
      "Model sonnet_output: None \t Accurate output: True\n",
      "Justification: Both the manual and automatic annotations for juvenile_leaf_hair_type are empty or null, indicating a lack of information or no hairs present, thus they can be considered equal.\n",
      "\n",
      "Groundtruth island_type: ['KAU']\n",
      "Model gpt_4o_output: [<Location.MOLOKAI: 'MOLOKAI'>] \t Accurate output: False\n",
      "Justification: The manually annotated island_type is 'KAU' which refers to the island of Kauai, whereas the automatically annotated island_type is 'MOLOKAI' which refers to the island of Molokai. These are two distinct islands in Hawaii, and therefore the annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is 'KAU', while the automatically annotated result is 'None', indicating that they are not equal.\n",
      "Model sonnet_output: [<Location.KAUAI: 'KAUAI'>] \t Accurate output: False\n",
      "Justification: The manual annotation 'KAU' and the automatic annotation 'KAUAI' are not exactly equal, although they refer to the same location, they have different formatting and level of specificity.\n",
      "\n",
      "Groundtruth hawaiian_name: {'lapalapa', '`Olapa'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains two Hawaiian names, 'lapalapa' and '`Olapa', while the automatically annotated result is None, indicating a lack of any extracted names. This discrepancy makes the two results non-equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains two values, 'lapalapa' and '`Olapa', while the automatically annotated result is None, indicating a lack of any annotated values. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated Hawaiian name is 'lapalapa' or '`Olapa', while the automatically annotated Hawaiian name is None, indicating a missing value. This difference indicates that the two annotations are not equal.\n",
      "\n",
      "Groundtruth stem_height: {'exmin': nan, 'min': 3.0, 'max': 4.5, 'exmax': nan, 'unit': 'm'}\n",
      "Model gpt_4o_output: min=None max=4.0 extreme_min=None extreme_max=None \t Accurate output: False\n",
      "Justification: The manually annotated result and the automatically annotated result for stem_height are not equal. The manual result has a minimum value of 3.0, while the automatic result has no minimum value. Although the automatic result has a maximum value of 4.0, which is close to the manual result's maximum of 4.5, the absence of a minimum value and the difference in maximum values make the two results non-synonymous.\n",
      "Model gpt_mini_output: min=3.0 max=4.0 extreme_min=None extreme_max=None \t Accurate output: False\n",
      "Justification: The manually annotated 'stem_height' has a maximum value of 4.5, while the automatically annotated 'stem_height' has a maximum value of 4.0, indicating a discrepancy between the two results.\n",
      "Model sonnet_output: min=3.0 max=4.5 extreme_min=None extreme_max=None \t Accurate output: True\n",
      "Justification: Both manual and automatic annotations for stem_height have the same minimum (3.0) and maximum (4.5) values, and both have missing or null values for extreme_min and extreme_max. The unit of measurement is also the same, 'm', in the manual annotation, and implied in the automatic annotation. Therefore, despite minor formatting differences, the two annotations are functionally equal.\n",
      "\n",
      "Groundtruth leaf_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: min=None max=35.0 extreme_min=None extreme_max=None \t Accurate output: False\n",
      "Justification: The manually annotated result has 'unit' specified as 'cm' while the automatically annotated result does not. Also, the keys for minimum and maximum values have different names ('min' vs 'min' and 'max' vs 'max') and representations ('nan' vs 'None') between the two results, indicating differences in formatting and potential nuances in the data.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The automatically annotated result is None, while the manually annotated result is a dictionary with specific keys, indicating a lack of equivalence between the two.\n",
      "Model sonnet_output: min=50.0 max=120.0 extreme_min=None extreme_max=None \t Accurate output: False\n",
      "Justification: The manual annotation has all values as nan (not a number), indicating missing or unknown data, while the automatic annotation has specific values for min, max, and None for extreme_min and extreme_max, indicating a significant difference in the level of detail and data availability between the two annotations.\n",
      "\n",
      "Groundtruth leaf_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: min=None max=15.0 extreme_min=None extreme_max=None \t Accurate output: False\n",
      "Justification: The manual and automatic annotations for leaf_width have different structures and values. The manual annotation has 'exmin', 'min', 'max', 'exmax' keys with nan values, while the automatic annotation has 'min', 'max', 'extreme_min', 'extreme_max' keys with different values. The units are also specified in the manual annotation ('cm') but not in the automatic annotation. Due to these differences in structure and values, the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has specific keys with 'nan' values, while the automatically annotated result is 'None', indicating a lack of information. Since 'nan' and 'None' represent different states of unknown or missing data, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even though the values are nan, whereas the automatically annotated result is None, indicating a complete absence of data or annotation, thus they cannot be considered equal.\n",
      "\n",
      "Groundtruth juvenile_leaf_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: min=None max=35.0 extreme_min=None extreme_max=None \t Accurate output: False\n",
      "Justification: The manually annotated result has all values as nan (not a number), while the automatically annotated result has a specific value for 'max' (35.0) and None for other values, indicating a difference in the annotation results.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', although all values except 'unit' are nan, indicating missing data. In contrast, the automatically annotated result is None, which implies a complete absence of data or annotation. Given the fundamental difference between having a structured but empty dataset (manual) and no dataset at all (automatic), the two annotations cannot be considered equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, whereas the automatically annotated result is None, indicating a complete absence of data. This suggests that the automatic extraction failed to provide any information for the 'juvenile_leaf_length' parameter, making the two results unequal.\n",
      "\n",
      "Groundtruth juvenile_leaf_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: min=None max=15.0 extreme_min=None extreme_max=None \t Accurate output: False\n",
      "Justification: The manual annotation has all values as 'nan' (not a number), while the automatic annotation has a specific value for 'max' (15.0) and 'None' for other values, indicating that they are not equal due to the presence of actual values in the automatic annotation and the lack of values in the manual annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated juvenile_leaf_width has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', although all values are 'nan', whereas the automatically annotated juvenile_leaf_width is None, indicating a complete absence of data or annotation, thus they cannot be considered equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even though the values are 'nan', whereas the automatically annotated result is 'None', indicating a lack of any annotation or data. This difference in structure and presence of data means the two results are not equal.\n",
      "\n",
      "Groundtruth leaflets_leaf_length: {'exmin': nan, 'min': 5.0, 'max': 12.0, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The automatically annotated result is None, indicating a lack of data or failure to extract the information, whereas the manually annotated result provides a range of values for leaflets_leaf_length. This discrepancy makes the two results unequal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains specific values for leaf length (min: 5.0, max: 12.0), while the automatically annotated result is None, indicating a lack of information. Therefore, the two annotations are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The automatically annotated result is None, which indicates that it was unable to extract any information, whereas the manual annotation provides a range of values for leaflets leaf length. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth leaflets_leaf_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manual annotation provides a dictionary with 'nan' values, indicating missing or unknown values, while the automatic annotation returns 'None', suggesting a complete absence of information or a failure in the extraction process. These two results are not equal due to the difference in how they represent the absence or lack of data.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for minimum, maximum, and unit, but all values are nan (not a number), indicating missing or undefined data. In contrast, the automatically annotated result is None, which implies a complete absence of data or a failure to extract any information. Given the fundamental difference between a structured but empty dataset (manual) and a null result (automatic), the two annotations cannot be considered equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined dictionary structure with 'nan' values, while the automatically annotated result is 'None', indicating a lack of data or undefined value. This difference in structure and content means the two annotations are not equal.\n",
      "\n",
      "Groundtruth petioles: {'exmin': nan, 'min': 3.0, 'max': 7.0, 'exmax': 10.0, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated petioles have a defined range with a minimum of 3.0 cm and a maximum of 7.0 cm, while the automatically annotated petioles are None, indicating a lack of extracted data. Therefore, the two annotations are not equal.\n",
      "Model gpt_mini_output: min=0.3 max=1.0 extreme_min=None extreme_max=None \t Accurate output: False\n",
      "Justification: The manually annotated petioles have a minimum of 3.0 cm, a maximum of 7.0 cm, and an extreme maximum of 10.0 cm, whereas the automatically annotated petioles have a minimum of 0.3, a maximum of 1.0, and no extreme values. The units also seem to differ, with the manual annotation in centimeters (cm) and the automatic annotation lacking a unit or possibly being in a different unit of measurement. Therefore, the two annotations are not equal.\n",
      "Model sonnet_output: min=3.0 max=7.0 extreme_min=None extreme_max=None \t Accurate output: False\n",
      "Justification: The manually annotated petioles have 'exmin' as nan and 'exmax' as 10.0, while the automatically annotated petioles have 'exmin' and 'exmax' as None. Although the 'min' and 'max' values are the same, the difference in 'exmin' and 'exmax' values makes the two annotations not equal.\n",
      "\n",
      "Groundtruth staminate_inflorescence_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but all numerical values are 'nan', indicating missing data. In contrast, the automatically annotated result is 'None', which implies a complete absence of data or annotation. Given the difference in representation and content, the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm'), while the automatically annotated result is None, indicating a lack of information. Additionally, the manual result has nan values for 'exmin', 'min', 'max', and 'exmax', which may indicate missing or unknown values, whereas the automatic result does not provide any information at all. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manual annotation has a specific unit 'mm' and keys for various values, whereas the automatic annotation is None, indicating a lack of information. This discrepancy suggests that the two results are not equal.\n",
      "\n",
      "Groundtruth staminate_inflorescence_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and a structure with various keys ('exmin', 'min', 'max', 'exmax'), whereas the automatically annotated result is None, indicating a lack of information or a failed extraction. These two results are fundamentally different in terms of content and structure.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'unit' specified as 'mm', whereas the automatically annotated result is 'None', indicating a lack of information or failure to annotate. This discrepancy suggests the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but all other values are either None or nan, whereas the automatically annotated result is entirely None, indicating a lack of information. Given the presence of a unit in the manual annotation, it suggests some level of processing or consideration, whereas the automatic result shows no such indication. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth pistillate_inflorescence_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has specific keys with nan values, while the automatically annotated result is None, indicating a lack of information. Although both results imply missing or unknown data, the difference in representation (nan vs None) and structure (dictionary vs None) makes them non-equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is 'None', indicating a lack of information or a failed annotation process. This discrepancy suggests that the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit of 'mm' but all other values are nan, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "\n",
      "Groundtruth pistillate_inflorescence_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit of 'mm' but all other values are None or nan, while the automatically annotated result is None. This indicates that the two results are not equal, as the manually annotated result provides some information about the unit, but the automatically annotated result provides no information at all.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit of 'mm', while the automatically annotated result is 'None', indicating a lack of information. This discrepancy suggests that the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and a structure with multiple fields, while the automatically annotated result is None, indicating a lack of information. This suggests that the automatic extraction failed to provide a comparable result, making the two annotations non-equal.\n",
      "\n",
      "Groundtruth inflorescence_flower_length: {'exmin': nan, 'min': nan, 'max': 100.0, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined maximum value of 100.0 mm, while the automatically annotated result is None, indicating a lack of information. Therefore, the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined maximum value of 100.0 mm, while the automatically annotated result is None, indicating a lack of information. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manual annotation provides a range with a maximum value of 100.0 mm, while the automatic annotation is None, indicating a lack of extracted information. Due to this significant difference, the two annotations cannot be considered equal.\n",
      "\n",
      "Groundtruth inflorescence_flower_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') while the automatically annotated result is None, indicating a lack of information. Therefore, the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit (mm) but all values are nan, whereas the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the complete absence of information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') while the automatic result is None, indicating a lack of information. Although both results have missing or undefined values, the presence of a unit in the manual annotation suggests that it is not entirely equivalent to the automatic result, which has no information.\n",
      "\n",
      "Groundtruth flower_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated flower_length has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated flower_length is None, indicating a complete absence of information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated flower_length has a specific unit ('cm') but all values are nan, whereas the automatically annotated flower_length is None, indicating a lack of information. These two annotations are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated flower_length has a defined structure with fields like 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated flower_length is None, indicating a lack of any annotation or value.\n",
      "\n",
      "Groundtruth flower_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'cm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manual annotation contains a dictionary with keys, even if the values are nan, while the automatic annotation is None, indicating a lack of any annotation, making them not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated flower_width has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', despite all values being nan, whereas the automatically annotated flower_width is None, indicating a complete absence of data or structure.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated flower_width has a defined structure with fields like 'exmin', 'min', 'max', 'exmax', and 'unit', even though all values are nan, whereas the automatically annotated flower_width is None, indicating a complete absence of information.\n",
      "\n",
      "Groundtruth rachis_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit (mm) but all values are nan, whereas the automatically annotated result is None, indicating a lack of information. These two results are not equal due to their different representations of missing or unspecified data.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated rachis_length has a defined structure with keys, but all values are nan, whereas the automatically annotated rachis_length is None, indicating a complete absence of information. This disparity in structure and content indicates they are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result and the automatically annotated result are not equal. The manually annotated result provides a dictionary with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', although all values are nan, whereas the automatically annotated result is None, indicating a lack of any information.\n",
      "\n",
      "Groundtruth rachis_diameter: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated rachis_diameter has specific keys such as 'exmin', 'min', 'max', 'exmax', 'unit' whereas the automatically annotated rachis_diameter is None, indicating a lack of any annotation, therefore they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result and the automatically annotated result for rachis_diameter are not equal. The manually annotated result is a dictionary with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', all of which are either nan or 'mm', while the automatically annotated result is None, indicating a lack of any annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is a dictionary with specific keys, although all values are 'nan', while the automatically annotated result is 'None', indicating a lack of data or a failed annotation process. These two results are not equal due to their different data types and structures.\n",
      "\n",
      "Groundtruth head_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated head_length has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated head_length is None, indicating a complete absence of annotation or data.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated head_length has a specified unit ('mm') but all other values are nan, whereas the automatically annotated head_length is None, indicating a lack of any annotation or value. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated head_length has a defined unit ('mm') but None values for the other fields, while the automatically annotated head_length is None, indicating a lack of any annotation. This suggests that the two results are not equal, as one has some information (the unit) and the other has no information.\n",
      "\n",
      "Groundtruth head_diameter: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated head_diameter has a dictionary with keys, but all values except 'unit' are nan, while the automatically annotated head_diameter is None, indicating a lack of any data. These two results are not equal due to the presence of a dictionary with some information in the manual annotation versus the complete absence of data in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manual annotation has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values for the extremities are nan, whereas the automatically annotated head_diameter is None, indicating a lack of any annotation or value.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is a dictionary with NaN values, while the automatically annotated result is None. These are fundamentally different data types and structures, indicating that the automatic extraction process did not produce a comparable result.\n",
      "\n",
      "Groundtruth bur_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated bur_length has a specific unit ('mm') and some defined keys, whereas the automatically annotated bur_length is None, indicating a complete absence of information. This suggests that the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated bur_length has a defined unit ('mm') and empty values for 'exmin', 'min', 'max', and 'exmax', whereas the automatically annotated bur_length is None, indicating a lack of any information. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated bur_length has a defined structure with 'unit' specified as 'mm', whereas the automatically annotated bur_length is None, indicating a lack of information. Despite both having missing or undefined values for 'exmin', 'min', 'max', and 'exmax', the presence of a 'unit' in the manual annotation and the complete absence of any information in the automatic annotation suggests they are not equal.\n",
      "\n",
      "Groundtruth tepal_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') but all values are nan (Not a Number), while the automatically annotated result is None, indicating a lack of information. Due to the absence of any comparable values and the presence of different representations of missing data, the two results cannot be considered equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and NaN values for 'exmin', 'min', 'max', 'exmax', whereas the automatically annotated result is None, indicating a lack of information. This suggests that the two results are not equal due to the presence of unit information in the manual annotation and the complete absence of information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated result is None, indicating a complete absence of information.\n",
      "\n",
      "Groundtruth staminate_tepal_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result provides a dictionary with specific keys (exmin, min, max, exmax, unit) even though the values are None or nan, whereas the automatically annotated result is simply None, indicating a lack of information. The presence of the dictionary structure in the manual annotation implies some level of processing or consideration for the 'staminate_tepal_length' characteristic, which is not reflected in the automatic annotation. Therefore, they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and a structure with various fields (exmin, min, max, exmax), whereas the automatically annotated result is None, indicating a lack of information or a failure to extract the data. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit of 'mm' but all other values are None or NaN, whereas the automatically annotated result is simply None, indicating a lack of information. These two results are not equal because one provides some information about the unit of measurement, while the other provides no information at all.\n",
      "\n",
      "Groundtruth pistillate_tepal_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for minimum and maximum values, as well as a unit of measurement, whereas the automatically annotated result is None, indicating a lack of Any extracted information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') and a specific structure, while the automatically annotated result is 'None', indicating a lack of information or a failed extraction, thus they are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and a structure with various fields, while the automatically annotated result is 'None', indicating a lack of information or a failure to extract the data. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth ray_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys, albeit with nan values, while the automatically annotated result is None, indicating a lack of data or annotation. This difference in structure and content means the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated ray_length has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are.nan, whereas the automatically annotated ray_length is None, indicating a lack of any annotation or value.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with values, while the automatically annotated result is None, indicating a lack of information or a failure to extract the data. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth ray_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result and the automatically annotated result are not equal because the manually annotated result is a dictionary with specific keys and values, while the automatically annotated result is None, indicating a lack of information or a failure to extract the data.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is None, indicating a lack of any annotation. This difference in structure and content means the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has specific keys with 'nan' values, while the automatically annotated result is 'None', indicating a lack of information. These two results are not equal due to the presence of keys in the manual result and the absence of any value in the automatic result.\n",
      "\n",
      "Groundtruth florets_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit (mm) but all values are nan, whereas the automatically annotated result is None, indicating a lack of information. These two results are not synonymous, as one provides some structural information (unit) while the other provides no information at all.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') while the automatically annotated result is 'None', indicating a lack of information. Despite both having 'nan' values for the numerical fields, the presence of a unit in the manual annotation and its absence in the automatic annotation suggests they are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit ('mm') but all other values are 'nan', while the automatically annotated result is 'None', indicating a lack of information. The presence of a unit in the manual annotation and the absence of any information in the automatic annotation suggest they are capturing different aspects of the data, thus they are not equal.\n",
      "\n",
      "Groundtruth involucre_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but all other values are nan, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result and the automatically annotated result are not equal because the manually annotated result is a dictionary with specific keys (exmin, min, max, exmax, unit) even if some of the values are nan, while the automatically annotated result is None.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but all other values are 'nan', while the automatically annotated result is 'None', indicating a lack of information. These two annotations are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "\n",
      "Groundtruth involucre_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated involucre_width has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, even though the values are nan or None, whereas the automatically annotated involucre_width is None, indicating a lack of any structure or information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') and a structured format with keys for 'exmin', 'min', 'max', 'exmax', whereas the automatically annotated result is None, indicating a lack of information. This discrepancy makes the two results non-equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated value has a defined unit ('mm') and a structured format with keys for extreme minimum, minimum, maximum, and extreme maximum, whereas the automatically annotated value is None, indicating a lack of information or a failure to extract a value.\n",
      "\n",
      "Groundtruth staminate_involucre_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'unit' specified as 'mm', whereas the automatically annotated result is None, indicating a lack of information. Therefore, the two results cannot be considered equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but empty or null values for the range, while the automatically annotated result is None, indicating a complete lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit of 'mm' but no actual measurements, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "\n",
      "Groundtruth pistilate_involucre_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but undefined values, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm'), while the automatically annotated result is 'None', indicating a lack of information. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit of measurement ('mm') but lacks actual values, whereas the automatically annotated result is entirely null, indicating a lack of information; thus, they do not convey the same information.\n",
      "\n",
      "Groundtruth bract_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit of 'mm' but all other values are 'nan', while the automatically annotated result is 'None', indicating a lack of information. Given the absence of actual values in both results, but the presence of a unit in the manual annotation, they cannot be considered equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated bract_length contains specific keys with NaN values, while the automatically annotated bract_length is None, indicating a lack of information. The presence of NaN values in the manual annotation suggests that the data was expected but not available, whereas the automatic annotation's None value implies that no data was extracted or available. Therefore, the two annotations are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') but all values are null (nan), while the automatically annotated result is None, indicating a lack of information. These two results are not equal because the manual annotation provides some information about the unit, whereas the automatic annotation does not.\n",
      "\n",
      "Groundtruth bract_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated bract_width has a defined unit ('mm') but all other values are 'nan' (not a number), while the automatically annotated bract_width is 'None', indicating a lack of any annotation. These two results are not equal due to their different states of annotation (partial vs none).\n",
      "Error processing model gpt_mini_output for key bract_width: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': 'final_result{\"are_equal\": false, \"justification\": \"The manually annotated bract_width has a specific structure with keys for \\'exmin\\', \\'min\\', \\'max\\', \\'exmax\\', and \\'unit\\', even though all numerical values are nan, whereas the automatically annotated bract_width is None, indicating a complete absence of information. Therefore, the two annotations are not equal.\"}</function>'}}\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated bract_width has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated bract_width is None, indicating a complete absence of information or annotation.\n",
      "\n",
      "Groundtruth bract_lower_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, whereas the automatically annotated result is 'None', indicating a complete absence of information. This suggests that the automatic annotation failed to extract any data for 'bract_lower_length', making it unequal to the manual annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but all numerical values are 'nan', whereas the automatically annotated result is 'None', indicating a lack of any annotation. These two results are not equal due to the presence of a unit in the manual annotation and the complete absence of any annotation in the automatic result.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains a dictionary with specific keys, including 'exmin', 'min', 'max', 'exmax', and 'unit', even if all the values are nan, while the automatically annotated result is None, indicating a complete absence of data. This difference in structure and content makes the two annotations non-equal.\n",
      "\n",
      "Groundtruth bract_outer_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but no actual values, while the automatically annotated result is None, indicating a lack of information. These results are not equal as one provides some information about the unit, while the other provides no information at all.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit of 'mm' but missing values for 'min' and 'max', while the automatically annotated result is 'None', indicating that there is no equivalent or comparable information between the two annotations.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm'), while the automatically annotated result is None, indicating a lack of information. Additionally, the manually annotated result has specific keys ('exmin', 'min', 'max', 'exmax') with values, whereas the automatically annotated result does not. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth bracteoles_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for minimum and maximum values, although the values are null or nan, whereas the automatically annotated result is None, indicating a complete absence of information. This suggests that the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit of measurement ('mm'), whereas the automatically annotated result is None, indicating a lack of information. This discrepancy suggests that the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated bracteoles_length has a defined unit ('mm') but empty or null values for 'exmin', 'min', 'max', 'exmax', whereas the automatically annotated bracteoles_length is None, indicating a complete absence of information. Therefore, the two results are not equal due to differences in their structure and content.\n",
      "\n",
      "Groundtruth bracteoles_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are None or nan, whereas the automatically annotated result is simply None, indicating a lack of any annotation or structure. This discrepancy suggests they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit (mm) and a structure with keys for minimum and maximum values, whereas the automatically annotated result is None, indicating a lack of information. This discrepancy suggests that the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manual annotation has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatic annotation is None, indicating a lack of any extracted information. This fundamental difference in structure and content means the two annotations are not equal.\n",
      "\n",
      "Groundtruth pedicel_length: {'exmin': nan, 'min': 3.0, 'max': 6.0, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated pedicel_length has specific values for 'min' and 'max', whereas the automatically annotated pedicel_length is None, indicating a lack of data or annotation. This significant difference in content and structure means the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a range of values (min: 3.0, max: 6.0) with a specified unit (mm), while the automatically annotated result is None, indicating a lack of any extracted information. Therefore, the two results are not equal.\n",
      "Model sonnet_output: min=3.0 max=6.0 extreme_min=None extreme_max=None \t Accurate output: True\n",
      "Justification: The manually annotated and automatically annotated results for pedicel_length have the same minimum and maximum values (3.0 and 6.0 respectively), and both have undefined or null extreme minimum and maximum values. The unit 'mm' in the manual annotation is implied in the automatic annotation as well, given the context. Thus, the two results can be considered equal.\n",
      "\n",
      "Groundtruth pedicel_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated pedicel_width has specific keys with values of None or nan, while the automatically annotated pedicel_width is None, indicating a lack of any annotation. This suggests that the two annotations are not equal, as one has some structure and the other has no information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated pedicel_width has a defined unit ('mm') but NaN values for 'min' and 'max', whereas the automatically annotated pedicel_width is None, indicating a lack of any annotation. These two results are not equal due to the absence of any comparable values in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains a dictionary with keys for minimum and maximum values, although the values themselves are None or nan, indicating unknown or missing data. In contrast, the automatically annotated result is None, which implies a complete lack of information. Given the difference in structure and content, the two results are not equal.\n",
      "\n",
      "Groundtruth staminate_pedicel_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for minimum and maximum values, as well as a unit, whereas the automatically annotated result is None, indicating a lack of information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but empty or null values for the range, while the automatically annotated result is completely null (None), indicating a lack of information. These are not equal as one provides some information (the unit) and the other provides no information.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for minimum and maximum values, albeit with NaN values, while the automatically annotated result is None, indicating a lack of information or failure to annotate.\n",
      "\n",
      "Groundtruth pistillate_pedicel_width: {'exmin': nan, 'min': None, 'max': None, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit of 'mm', while the automatically annotated result is None, indicating a lack of information. Although both results do not have specific minimum or maximum values, the presence of a unit in the manual annotation and the absence of any information in the automatic annotation suggest that they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result provides a dictionary with specific keys (exmin, min, max, exmax, unit) even if some values are None or nan, whereas the automatically annotated result is None, indicating a lack of any annotation or data. This difference in structure and content indicates they are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and undefined minimum and maximum values, while the automatically annotated result is None, indicating a lack of information. These two results cannot be considered equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "\n",
      "Groundtruth pistillate_pedicel_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result provides a dictionary with specific keys (exmin, min, max, exmax, unit) for pistillate_pedicel_length, even though the values are None or nan, whereas the automatically annotated result is None, indicating a lack of any annotation or information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' fields, whereas the automatically annotated result is None, indicating a lack of data or annotation. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but all other values are undefined (None or nan), while the automatically annotated result is completely undefined (None). Although both results are incomplete, the presence of a defined unit in the manual annotation and the complete absence of any information in the automatic annotation makes them non-equivalent.\n",
      "\n",
      "Groundtruth hypanthium_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with various fields (exmin, min, max, exmax, unit), even if some values are None or nan, whereas the automatically annotated result is simply None, indicating a lack of any annotation or data. This difference in structure and content indicates they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, whereas the automatically annotated result is None, indicating a lack of data or a failure in the extraction process. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with fields like 'exmin', 'min', 'max', 'exmax', and 'unit', even if some values are None or nan, whereas the automatically annotated result is simply None, indicating a lack of any annotation or data.\n",
      "\n",
      "Groundtruth hypanthium_width: {'exmin': nan, 'min': None, 'max': None, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with various keys (exmin, min, max, exmax, unit), while the automatically annotated result is None, indicating a lack of information. Although both results contain null or None values, their overall structure and content are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific format with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if some values are None or nan, whereas the automatically annotated result is simply None, indicating a lack of any annotation or data, thus the two are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if some values are None or nan, whereas the automatically annotated result is simply None, indicating a lack of any annotation or structure.\n",
      "\n",
      "Groundtruth peduncle_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manual annotation contains a dictionary with keys for minimum and maximum values, although they are all nan, and a unit, whereas the automatic annotation is None, indicating a lack of extracted information. This difference in structure and content means the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has specific keys with nan values, while the automatically annotated result is None, indicating a missing value. These two results are not equal due to the difference in representation and potential for the manual annotation to be updated with actual values in the future.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated peduncle_length has a defined structure with keys, even if the values are nan, whereas the automatically annotated peduncle_length is None, indicating a complete absence of information.\n",
      "\n",
      "Groundtruth peduncle_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') while the automatically annotated result is 'None', indicating a lack of extracted information. This difference in content and formatting makes the two annotations non-equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated peduncle_width has a defined unit ('mm') but all other values are None or nan, while the automatically annotated peduncle_width is None, indicating a lack of any annotation. Given the absence of comparable values and the presence of a defined unit in the manual annotation, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated peduncle_width has values, although they are None or nan, indicating that some consideration was given to this characteristic. On the other hand, the automatically annotated peduncle_width is None, indicating that this characteristic was not considered or extracted. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth staminate_peduncle_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and null values for 'exmin', 'min', 'max', 'exmax', while the automatically annotated result is None, indicating a lack of information or a failed annotation process. These two results are not synonymous or equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and placeholder values for 'exmin', 'min', 'max', 'exmax', while the automatically annotated result is 'None', indicating a lack of information or no annotation, making them unequal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but all numeric values are nan, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "\n",
      "Groundtruth staminate_peduncle_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') but all numeric values are 'nan', whereas the automatically annotated result is 'None', indicating a lack of information. Considering linguistic and formatting nuances, these two results are not equal due to the presence of a unit in the manual annotation and the absence of any value in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') whereas the automatically annotated result is None, indicating a lack of information or a failed extraction, thus the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') associated with it, whereas the automatically annotated result is None, indicating a complete absence of information. This difference in content and structure makes the two annotations non-equivalent.\n",
      "\n",
      "Groundtruth pistillate_peduncle_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', but all values except 'unit' are nan, indicating missing or unspecified data. In contrast, the automatically annotated result is None, indicating a complete absence of data or annotation. Given the difference in structure and content, the two results cannot be considered equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit ('mm') but all other values are 'nan', while the automatically annotated result is 'None', indicating a complete absence of information. These two results are not equal due to the presence of a unit in the manual annotation and the complete lack of information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even though all values except 'unit' are nan, whereas the automatically annotated result is None, indicating a complete absence of data. This discrepancy suggests that the two results are not equal.\n",
      "\n",
      "Groundtruth pistillate_peduncle_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and empty values for 'exmin', 'min', 'max', 'exmax', while the automatically annotated result is None, indicating a lack of information. The presence of a unit in the manual annotation and the absence of any value in the automatic annotation suggest they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but all other values are nan, while the automatically annotated result is None, indicating a lack of information. These results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') but all values are 'nan', indicating missing data. In contrast, the automatically annotated result is 'None', which suggests a lack of information or a failure to extract the data. Given the differences in formatting and content, the two results cannot be considered equal.\n",
      "\n",
      "Groundtruth spathe_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated spathe_length has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated spathe_length is None, indicating a complete absence of information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated spathe_length has a defined unit ('mm') with all numeric values as 'nan', whereas the automatically annotated spathe_length is 'None', indicating a lack of annotation. This difference in structure and content suggests they are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated spathe_length has a defined unit ('mm') but all values are 'nan', whereas the automatically annotated spathe_length is 'None', indicating a complete absence of information. These two annotations are not equal due to the difference in the presence of a unit in the manual annotation and the lack of any information in the automatic annotation.\n",
      "\n",
      "Groundtruth spathe_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated spathe_width has a defined unit ('mm') but all values are 'nan', whereas the automatically annotated spathe_width is None, indicating a complete absence of data. Given the presence of formatting nuances (unit) in the manual annotation, it does not match the null automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated spathe_width has a defined unit ('mm') but all other values are 'nan', whereas the automatically annotated spathe_width is 'None', indicating a lack of any information. These two results are not equal due to the difference in the presence of a unit in the manual annotation and the complete absence of information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated spathe_width has a defined unit ('mm') but all other values are nan, whereas the automatically annotated spathe_width is None, indicating a lack of any information. Given the fundamental difference between having some structure (even if with nan values) and having no value at all, these annotations cannot be considered equal.\n",
      "\n",
      "Groundtruth spadix_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated spadix_length has a defined unit ('mm') and a structure with keys for minimum and maximum values, whereas the automatically annotated spadix_length is None, indicating a complete absence of information. This significant difference in content and structure makes the two annotations non-equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manual annotation provides a dictionary with specific keys for minimum and maximum values, as well as a unit, even if the values are None or nan. In contrast, the automatic annotation is None, indicating a complete absence of information. Therefore, the two annotations are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated spadix_length has a specified unit of 'mm' but all other values are null or undefined, whereas the automatically annotated spadix_length is None, indicating a lack of any annotation or value. Therefore, the two annotations are not equal due to the presence of a unit in the manual annotation and the complete absence of any value or annotation in the automatic one.\n",
      "\n",
      "Groundtruth perianth_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_length has a defined unit ('mm') but missing numeric values, while the automatically annotated perianth_length is None, indicating a lack of any information. Given these differences, the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_length has specific keys (exmin, min, max, exmax, unit) with nan values and a unit of 'mm', whereas the automatically annotated perianth_length is None, indicating a lack of any annotation or value. This difference in structure and content means the two annotations are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_length has specific keys with nan values, while the automatically annotated perianth_length is None, indicating a lack of any information, making them non-equal.\n",
      "\n",
      "Groundtruth perianth_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_width has specific keys with nan values, while the automatically annotated perianth_width is None, indicating a lack of information. These two results are not equal due to the absence of any comparable values in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_width has a specified unit of 'mm', but all values are nan (not a number), while the automatically annotated perianth_width is None, indicating a lack of information. Since the manually annotated result provides some information (the unit), even if the values are nan, it is not equal to the automatically annotated result, which provides no information.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_width has a defined structure with keys, even if the values are nan, whereas the automatically annotated perianth_width is None, indicating a lack of any value or structure.\n",
      "\n",
      "Groundtruth perianth_outer_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_outer_length has a defined unit ('mm') but empty or null values for the range, whereas the automatically annotated perianth_outer_length is completely null, indicating no extracted information. These two annotations are not equivalent due to the presence of a unit in the manual annotation and the complete absence of any information in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_outer_length has a specified unit of 'mm' but all other values are None or nan, whereas the automatically annotated perianth_outer_length is None. This indicates that the automatic annotation did not provide any information, while the manual annotation provided some structure, but no actual values. Therefore, the two annotations are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit (mm) and structure, while the automatically annotated result is None, indicating a lack of information or failure to extract the data, thus they are not equal.\n",
      "\n",
      "Groundtruth perianth_outer_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_outer_width has a defined unit ('mm') but empty or undefined values, while the automatically annotated perianth_outer_width is None, indicating a lack of any annotation or value. These are not equal because one has some level of annotation detail (unit), while the other (automatic) has no detail at all.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit (mm) but no actual value, while the automatically annotated result is completely absent (None), indicating a lack of extracted information. These two results cannot be considered equal due to the absence of any comparable values in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but missing numerical values, while the automatically annotated result is None, indicating a complete lack of information. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth perianth_inner_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_inner_length has some properties defined such as 'unit' being 'mm', whereas the automatically annotated perianth_inner_length is None, indicating a lack of any information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but lacks actual values, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_inner_length has a unit of 'mm' with some undefined values, whereas the automatically annotated perianth_inner_length is None, indicating a lack of information. These two results cannot be considered equal due to the presence of a unit in the manual annotation and the absence of any value in the automatic annotation.\n",
      "\n",
      "Groundtruth perianth_inner_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') but all width values are undefined (None or nan), whereas the automatically annotated result is None, indicating a lack of information. While both results lack specific width values, the manual annotation provides some context with the unit, which the automatic annotation does not. Thus, they are not equal due to the difference in provided context.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit 'mm' but all other values are null or undefined, while the automatically annotated result is entirely null, indicating a lack of information. Given the absence of comparable values and the difference in the presence of a unit in the manual annotation, the two results cannot be considered equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_inner_width contains details such as 'exmin', 'min', 'max', 'exmax', and 'unit', even if some values are None or nan, whereas the automatically annotated perianth_inner_width is entirely None, indicating a lack of any extracted or provided information.\n",
      "\n",
      "Groundtruth perianth_tube_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and a structure with keys for minimum and maximum values, while the automatically annotated result is None, indicating a lack of information. This difference in structure and content means the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_tube_length has a defined unit ('mm') but no actual values, while the automatically annotated perianth_tube_length is completely absent (None), indicating a lack of equivalence between the two.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_tube_length has a defined unit ('mm') but null or undefined values for the other parameters, whereas the automatically annotated perianth_tube_length is completely null, indicating that the automatic annotation failed to provide any information, so they cannot be considered equal.\n",
      "\n",
      "Groundtruth perianth_lobes_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit of 'mm' whereas the automatically annotated result is None, indicating a lack of information. Although both have undefined or missing values for the minimum and maximum lengths, the presence of a unit in the manual annotation and its absence in the automatic annotation, along with the automatic result being completely null, suggests they are not capturing the same information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result provides a dictionary with specific keys (exmin, min, max, exmax, unit) for perianth_lobes_length, whereas the automatically annotated result is None, indicating a lack of any information. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated perianth_lobes_length has a defined unit ('mm') but no specified values, while the automatically annotated perianth_lobes_length is None, indicating a lack of any annotation. These two results are not equal due to the absence of any annotation in the automatic result, which cannot be directly compared to the manual result with a defined unit.\n",
      "\n",
      "Groundtruth perianth_lobes_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but no specific values, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but with missing or undefined numerical values, whereas the automatically annotated result is completely undefined (None), indicating a lack of equivalence due to the absence of any comparable information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') and structure, while the automatically annotated result is None, indicating a lack of equivalence between the two.\n",
      "\n",
      "Groundtruth staminate_perianth_tube_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for minimum and maximum values, as well as a unit, whereas the automatically annotated result is None, indicating a lack of information. This difference in structure and content means the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but missing values, while the automatically annotated result is completely absent (None), indicating a lack of equivalent information.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but no values, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "\n",
      "Groundtruth pistillate_perianth_tube_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains a dictionary with specific keys (exmin, min, max, exmax, unit) whereas the automatically annotated result is None, indicating a lack of annotation. Despite the manually annotated values being None or nan, the presence of a structured dictionary suggests an attempt at annotation, whereas the automatic result shows no such attempt. Therefore, the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for minimum and maximum values, even if the values themselves are null or NaN, whereas the automatically annotated result is simply None, indicating a lack of any annotation or data.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit of 'mm' but with undefined values, while the automatically annotated result is None, indicating a lack of information. Given the absence of comparable values, the two results cannot be considered equal.\n",
      "\n",
      "Groundtruth pappus_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated pappus_length has a specified unit of 'mm', while the automatically annotated pappus_length is None, indicating a lack of information. This suggests that the two annotations are not equal, as one provides some information (the unit) and the other provides no information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manual annotation contains a dictionary with some values, albeit 'nan' and 'None', whereas the automatic annotation is 'None', indicating no value was extracted. Given the lack of equivalence in structure and content, the results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated pappus_length has a defined unit ('mm') but no actual values, while the automatically annotated pappus_length is None, indicating a complete lack of information. These two results are not equal because one provides some information (unit) while the other provides no information.\n",
      "\n",
      "Groundtruth umbellet_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated umbellet_length has a defined unit ('mm') but unspecified ranges, while the automatically annotated umbellet_length is completely undefined (None), indicating a lack of equivalent information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated umbellet_length has a specified unit of 'mm' but with no minimum or maximum values, whereas the automatically annotated umbellet_length is None, indicating a lack of any information. These two annotations are not equal due to the presence of a unit in the manual annotation and the complete absence of information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but empty or null values for the other fields, while the automatically annotated result is completely null (None), indicating a lack of information. These two results are not synonymous or equal due to the presence of a unit in the manual annotation and the complete absence of any annotation in the automatic result.\n",
      "\n",
      "Groundtruth labellum_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated labellum_length has a specified unit ('mm') but none of the other fields have values, whereas the automatically annotated labellum_length is None, indicating a lack of any annotation. These two results are not equal due to the presence of a unit in the manual annotation and the complete absence of any annotation in the automatic result.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated labellum_length has a specified unit of 'mm' but all other values are None or nan, whereas the automatically annotated labellum_length is None, indicating a lack of information. Due to the presence of a unit in the manual annotation, it provides more information than the automatic annotation, even if the values are missing or undefined.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with fields such as 'exmin', 'min', 'max', 'exmax', and 'unit', even though some values are None or nan. In contrast, the automatically annotated result is None, indicating a complete absence of information. This fundamental difference in structure and content makes the two results non-equal.\n",
      "\n",
      "Groundtruth labellum_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated labellum_width has a defined unit ('mm') but none of its numerical values are populated, whereas the automatically annotated labellum_width is None, indicating a complete absence of information. Therefore, they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated labellum_width has a defined unit ('mm') but all other values are None or nan, while the automatically annotated labellum_width is entirely None, indicating a lack of any extracted information. Therefore, the two annotations are not equal due to the presence of a unit in the manual annotation and the complete absence of information in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated labellum_width has a specified unit ('mm') but all its numeric values are None or nan, whereas the automatically annotated labellum_width is None, indicating a lack of information. The presence of a unit in the manual annotation and its complete absence in the automatic annotation suggest they are capturing different aspects or levels of detail, thus they are not equal.\n",
      "\n",
      "Groundtruth calyx_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated calyx_length has a defined unit ('mm') but all values are nan (not a number), while the automatically annotated calyx_length is None, indicating a complete absence of information. This discrepancy suggests that the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated calyx_length has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated calyx_length is None, indicating a complete absence of data or annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated calyx_length has a defined structure with keys, although all values except 'unit' are nan, whereas the automatically annotated calyx_length is None, indicating a complete absence of data. This absence of data cannot be considered equal to the manual annotation, even if the manual annotation contains nan values.\n",
      "\n",
      "Groundtruth calyx_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated calyx_width contains specific keys with nan values, while the automatically annotated calyx_width is None, indicating a lack of information. These two annotations are not equal, as one contains empty values and the other contains no information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated calyx_width has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated calyx_width is None, indicating a complete absence of information.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are 'nan', whereas the automatically annotated result is 'None', indicating a complete absence of information. This discrepancy suggests that the two results are not equal.\n",
      "\n",
      "Groundtruth calyx_teeth_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even though the values are nan or None, whereas the automatically annotated result is simply None, indicating a lack of any annotation or structure. Therefore, the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated calyx_teeth_length has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, whereas the automatically annotated calyx_teeth_length is None, indicating a lack of any annotation or value. This discrepancy suggests that the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated value has a defined unit ('mm') but all other values are nan or None, while the automatically annotated value is None. This suggests that the automatic annotation was unable to extract any information for 'calyx_teeth_length', whereas the manual annotation provided some contextual information (the unit) even if the actual values are missing.\n",
      "\n",
      "Groundtruth calyx_teeth_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated calyx_teeth_width contains a dictionary with specific keys, whereas the automatically annotated calyx_teeth_width is None, indicating a lack of equivalent information.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but null or undefined values for 'exmin', 'min', 'max', 'exmax', whereas the automatically annotated result is entirely null, indicating a lack of overlap or equivalence between the two.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for minimum and maximum values, albeit with null or nan values, whereas the automatically annotated result is None, indicating a complete lack of information. This discrepancy suggests that the two results are not equal.\n",
      "\n",
      "Groundtruth calyx_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') but all other values are 'nan', whereas the automatically annotated result is 'None', indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is None, indicating a complete lack of information. This suggests that the automatic annotation failed to provide any relevant data, making it impossible for the two results to be equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated and automatically annotated results for calyx_lobes_length are not equal because the manually annotated result is a dictionary with specific keys, even if the values are nan, while the automatically annotated result is None, indicating a lack of information or a failed annotation process.\n",
      "\n",
      "Groundtruth calyx_lobes_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated result is None, indicating a lack of any annotation or data.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result and the automatically annotated result are not equal because the manually annotated result has a unit specified ('mm'), while the automatically annotated result is None, indicating a complete lack of information.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated result is None, indicating a complete absence of information.\n",
      "\n",
      "Groundtruth upper_calyx_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with various keys such as 'exmin', 'min', 'max', 'exmax', and 'unit', although most values are nan or None, whereas the automatically annotated result is simply None, indicating a lack of any annotation or value, thus they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if some values are nan or None, whereas the automatically annotated result is simply None, indicating a lack of any annotation or structure.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if some values are nan or None, whereas the automatically annotated result is None, indicating a complete absence of data or annotation.\n",
      "\n",
      "Groundtruth lower_calyx_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', while the automatically annotated result is None, indicating a lack of information or a failure to extract the data. Therefore, the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', although all values are 'nan', whereas the automatically annotated result is 'None', indicating a lack of any annotation or value. This difference in structure and content indicates they are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, while the automatically annotated result is None, indicating a lack of any annotation or value, thus they are not equal.\n",
      "\n",
      "Groundtruth inner_calyx_lobes_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated inner_calyx_lobes_length has a defined unit ('mm') but NaN values for 'min' and 'max', while the automatically annotated inner_calyx_lobes_length is None, indicating a lack of any annotation. Therefore, the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and structure, while the automatically annotated result is 'None', indicating a lack of information or a failed extraction, making them non-equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but empty or undefined values for the other parameters, whereas the automatically annotated result is completely undefined (None), indicating a lack of information. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth inner_calyx_lobes_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') and a specific structure, while the automatically annotated result is None, indicating a lack of information or a failure in the automated process.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if some values are None or nan, whereas the automatically annotated result is None, indicating a lack of any annotation or data, thus they are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with specific keys, whereas the automatically annotated result is None, indicating a lack of data or annotation. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth outer_calyx_lobes_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but no numeric values, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the complete absence of information in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but lacks actual values, while the automatically annotated result is entirely None, indicating a fundamental difference in their content and structure.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, whereas the automatically annotated result is None, indicating a complete absence of data. This difference in structure and content means the two results are not equal.\n",
      "\n",
      "Groundtruth outer_calyx_lobes_width: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys such as 'exmin', 'min', 'max', 'exmax', and 'unit', even if some of the values are None or nan, whereas the automatically annotated result is None, indicating a complete absence of data or annotation, thus they cannot be considered equal due to the fundamental difference in their structure and content.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys such as 'exmin', 'min', 'max', 'exmax', and 'unit', but all values are None or nan, indicating a lack of specific measurement data. In contrast, the automatically annotated result is None, indicating a complete absence of data. Given the difference in structure and content, the two results cannot be considered equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with fields like 'exmin', 'min', 'max', 'exmax', and 'unit', even if some values are None or nan, whereas the automatically annotated result is None, indicating a complete absence of data or annotation. This difference in structure and content means the two results are not equal.\n",
      "\n",
      "Groundtruth calyx_tube_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even though all numerical values are nan, whereas the automatically annotated result is None, indicating a complete absence of data or annotation, thus they cannot be considered equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, although all values are 'nan', whereas the automatically annotated result is 'None', indicating a complete absence of data or annotation, which cannot be considered equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit ('mm'), while the automatically annotated result is None, indicating a lack of information. Although both results contain missing or null values, the presence of a unit in the manual annotation and the absence of any value in the automatic annotation suggest they are not equal.\n",
      "\n",
      "Groundtruth calyx_tube_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit ('mm') but all other values are 'nan', while the automatically annotated result is 'None', indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the absence of any information in the automatic annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated calyx_tube_width has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated calyx_tube_width is None, indicating a complete absence of information.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result and the automatically annotated result are not equal because the manually annotated result has a dictionary with keys, whereas the automatically annotated result is None, indicating a lack of any annotation.\n",
      "\n",
      "Groundtruth male_calyx_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manual annotation contains a dictionary with keys for minimum, maximum, and unit values, albeit all being 'nan', while the automatic annotation is 'None', indicating a lack of any information. This discrepancy suggests the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, whereas the automatically annotated result is None, indicating a lack of any annotation or value. This suggests that the automated extraction failed to provide a meaningful result, making the two annotations non-equivalent.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has specific keys with 'nan' values, while the automatically annotated result is 'None', indicating a lack of information. These two representations of missing or undefined data are not equivalent, as 'nan' typically implies an unknown or missing numeric value, whereas 'None' suggests the absence of any value, including non-numeric ones.\n",
      "\n",
      "Groundtruth male_calyx_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has specific keys with 'nan' values, while the automatically annotated result is 'None', indicating a lack of information. The presence of keys in the manual result versus the absence of any value in the automatic result suggests they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with fields like 'exmin', 'min', 'max', 'exmax', and 'unit', even though all values except 'unit' are nan, whereas the automatically annotated result is None, indicating a lack of any annotation or value.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even though the values are 'nan', while the automatically annotated result is 'None', indicating a lack of any annotation or structure.\n",
      "\n",
      "Groundtruth male_calyx_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated result is None, indicating a lack of any annotation or value.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific format with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', but all values are 'nan', whereas the automatically annotated result is 'None', indicating a lack of annotation. These two results are not equal due to the fundamental difference in their structure and content.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are 'nan'. In contrast, the automatically annotated result is 'None', indicating a lack of any information. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth male_calyx_lobes_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, while the automatically annotated result is None, indicating a lack of any annotation or data.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for minimum, maximum, and unit, even if the values are nan, while the automatically annotated result is None, indicating a complete absence of information. This difference in structure and content means the two annotations are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', although all values are nan, while the automatically annotated result is None, indicating a lack of any annotation or value.\n",
      "\n",
      "Groundtruth female_calyx_length: {'exmin': nan, 'min': female_calyx_length_mm_min    NaN\n",
      "female_calyx_length_mm_min    NaN\n",
      "Name: 14, dtype: object, 'max': female_calyx_length_mm_max    NaN\n",
      "female_calyx_length_mm_max    NaN\n",
      "Name: 14, dtype: object, 'exmax': female_calyx_length_mm_exmax    NaN\n",
      "female_calyx_length_mm_exmax    NaN\n",
      "Name: 14, dtype: object, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for minimum, maximum, and unit, even though the values are NaN, whereas the automatically annotated result is None, indicating a complete absence of information. This difference in structure and content indicates that the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', although all values are NaN, whereas the automatically annotated result is None, indicating a lack of data. These two results are not equal due to their different formats and the presence of structure in the manual annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains NaN values but has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, while the automatically annotated result is None, indicating a lack of any extracted information. Due to this significant difference in structure and content, the two annotations cannot be considered equal.\n",
      "\n",
      "Groundtruth female_calyx_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, while the automatically annotated result is None, indicating a lack of any annotation or value.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even though the values are nan, whereas the automatically annotated result is None, indicating a complete absence of data or annotation. This discrepancy makes the two annotations non-equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is None, indicating a complete absence of data. This discrepancy suggests that the two results are not equal.\n",
      "\n",
      "Groundtruth female_calyx_lobes_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, while the automatically annotated result is None, indicating a complete absence of data. This difference in structure and content means the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated result is None, indicating a complete absence of data or annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', while the automatically annotated result is None, indicating a lack of any annotation or value. This significant difference in structure and content indicates that the two results are not equal.\n",
      "\n",
      "Groundtruth female_calyx_lobes_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit (mm) but all values are nan, whereas the automatically annotated result is None, indicating a lack of information. The presence of a unit in the manual annotation and the absence of any value in the automatic annotation suggest they are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is a dictionary with empty or 'nan' values, while the automatically annotated result is 'None', indicating a lack of any value. These two results are not equal due to their difference in data type and content.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', but all values are nan, whereas the automatically annotated result is None, indicating a complete absence of data. This discrepancy suggests that the two results are not equal.\n",
      "\n",
      "Groundtruth male_calyx_lobes_length_inner: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specified unit ('mm') but no actual measurement values, while the automatically annotated result is None, indicating that no information was extracted. Therefore, the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit (mm) and structure, while the automatically annotated result is None, indicating a lack of extracted information.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for minimum and maximum values, as well as a unit of measurement, whereas the automatically annotated result is None, indicating a lack of any extracted information.\n",
      "\n",
      "Groundtruth male_calyx_lobes_length_outer: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but undefined numeric values, whereas the automatically annotated result is None, indicating a lack of information. Therefore, the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is a dictionary with various keys, while the automatically annotated result is None, indicating a lack of information or a failed extraction. The two results are not equal due to the presence of a structured dictionary in the manual annotation and the absence of any data in the automatic annotation.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') but undefined values, whereas the automatically annotated result is entirely undefined (None), indicating a lack of equivalence between the two annotations.\n",
      "\n",
      "Groundtruth male_calyx_lobes_width_outer: {'exmin': None, 'min': male_calyx_lobes_width_outer_mm_min    NaN\n",
      "male_calyx_lobes_width_outer_mm_min    NaN\n",
      "Name: 14, dtype: object, 'max': male_calyx_lobes_width_outer_mm_max    NaN\n",
      "male_calyx_lobes_width_outer_mm_max    NaN\n",
      "Name: 14, dtype: object, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains specific keys such as 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is None, indicating a lack of any extracted information. Therefore, the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains specific keys such as 'exmin', 'min', 'max', 'exmax', and 'unit' with values, whereas the automatically annotated result is None, indicating a lack of any annotation. Therefore, the two annotations are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains specific keys such as 'exmin', 'min', 'max', 'exmax', and 'unit' with values, whereas the automatically annotated result is None, indicating a lack of any extracted information. Therefore, the two annotations are not equal.\n",
      "\n",
      "Groundtruth male_calyx_tube_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit of 'mm' whereas the automatically annotated result is None, indicating a lack of information. This discrepancy suggests that the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific format with keys for different values, but all values are nan, whereas the automatically annotated result is None, indicating a lack of information. The two results are not equal due to the difference in their structure and content.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') but all numerical values are nan, while the automatically annotated result is None, indicating a lack of information. These two results are not equal due to the presence of a unit in the manual annotation and the complete absence of information in the automatic annotation.\n",
      "\n",
      "Groundtruth female_calyx_lobes_length_inner: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is a dictionary with various keys, including 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is None. This indicates that the automatic annotation was unable to extract any relevant information for the 'female_calyx_lobes_length_inner' field, whereas the manual annotation provided a structured output, even if some values are missing (represented by None or nan). Therefore, the two results are not equal due to the significant difference in their structure and content.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') and a structure with various fields (exmin, min, max, exmax), while the automatically annotated result is None, indicating a lack of information or a failure to extract the data. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is a dictionary with keys for minimum and maximum values, although the values themselves are empty or null, whereas the automatically annotated result is None, indicating a complete lack of information. This difference in structure and content means the two annotations are not equal.\n",
      "\n",
      "Groundtruth female_calyx_lobes_length_outer: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result contains a dictionary with various keys such as 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is None, indicating a complete absence of data. This difference in structure and content means the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has specific keys with null or nan values, while the automatically annotated result is None, indicating a lack of information. The two results are not equal due to the difference in their structure and content.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for minimum and maximum values, although the values are nan, whereas the automatically annotated result is None, indicating a lack of any annotation or data.\n",
      "\n",
      "Groundtruth female_calyx_lobes_width_inner: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is a dictionary with specific keys and values, including 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is None, indicating a complete absence of annotation. This suggests that the automatic annotation process failed to extract or provide any relevant information for the 'female_calyx_lobes_width_inner' trait, making it fundamentally different from the manual annotation.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is a dictionary with specific keys, including 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is None, indicating a complete absence of information. This discrepancy suggests that the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific format with keys for minimum and maximum values, although they are empty or None, whereas the automatically annotated result is None, indicating a lack of data or incomplete annotation. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth female_calyx_lobes_width_outer: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has specific keys with 'None' or 'nan' values, while the automatically annotated result is 'None', indicating a lack of data. These two results are not equal because one has some structure, albeit with empty or null values, while the other has no structure at all.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result is a dictionary with various keys, while the automatically annotated result is None, indicating a lack of data or annotation. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with various keys (exmin, min, max, exmax, unit), whereas the automatically annotated result is None, indicating a lack of data or incomplete extraction. Therefore, the two results cannot be considered equal.\n",
      "\n",
      "Groundtruth inner_calyx_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if some values are None or nan, whereas the automatically annotated result is simply None, indicating a lack of any annotation or data.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manual annotation has a specific unit ('mm') and a structured format, while the automatic annotation is None, indicating a lack of information or a failed extraction, thus the two annotations are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated inner_calyx_length has a defined unit ('mm') while the automatically annotated result is None, indicating a lack of information. Therefore, the two results are not equal.\n",
      "\n",
      "Groundtruth outer_calyx_length: {'exmin': None, 'min': nan, 'max': nan, 'exmax': None, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with 'exmin', 'min', 'max', 'exmax', and 'unit' keys, whereas the automatically annotated result is None, indicating a lack of any annotation or value. This disparity suggests that the two results are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but no actual values, while the automatically annotated result is None, indicating a lack of any extracted information. Therefore, the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined unit ('mm') but empty or undefined values for the extremes, while the automatically annotated result is None, indicating a lack of extracted information. These two results do not convey the same information.\n",
      "\n",
      "Groundtruth corolla_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific unit ('mm') but all values are 'nan', while the automatically annotated result is 'None', indicating a lack of information. Since 'nan' and 'None' represent different types of missing information and the manually annotated result provides some context with the unit, the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has specific keys with 'nan' values, indicating missing or unknown data, while the automatically annotated result is 'None', which implies a complete absence of data or annotation. This difference in representation suggests that the two results are not equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a specific structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated result is None, indicating a lack of information or a failed annotation process. This difference in structure and content indicates that the two results are not equal.\n",
      "\n",
      "Groundtruth corolla_width: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n",
      "Model gpt_4o_output: None \t Accurate output: False\n",
      "Justification: The manually annotated corolla_width has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', but all values are 'nan'. In contrast, the automatically annotated corolla_width is 'None', indicating a complete absence of data. Given the difference in structure and content, the two annotations are not equal.\n",
      "Model gpt_mini_output: None \t Accurate output: False\n",
      "Justification: The manually annotated result has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', whereas the automatically annotated result is 'None', indicating a lack of information or a failure to annotate. This discrepancy makes the two results non-equal.\n",
      "Model sonnet_output: None \t Accurate output: False\n",
      "Justification: The manually annotated corolla_width has a defined structure with keys for 'exmin', 'min', 'max', 'exmax', and 'unit', even if the values are nan, whereas the automatically annotated corolla_width is None, indicating a complete absence of data or structure.\n",
      "\n",
      "Groundtruth corolla_tube_length: {'exmin': nan, 'min': nan, 'max': nan, 'exmax': nan, 'unit': 'mm'}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/groq/_base_client.py:1606\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1606\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m         user_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mmanually annotated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_value\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mautomatically annotated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauto_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m             result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m validation_agent\u001b[38;5;241m.\u001b[39mrun(user_prompt)  \u001b[38;5;66;03m# Ensure `await` is used correctly\u001b[39;00m\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauto_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Accurate output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mare_equal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJustification: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mjustification\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print the justification\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/pydantic_ai/agent.py:340\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, user_prompt, message_history, model, deps, model_settings, usage_limits, usage, result_type, infer_name)\u001b[0m\n\u001b[1;32m    332\u001b[0m     start_node \u001b[38;5;241m=\u001b[39m _agent_graph\u001b[38;5;241m.\u001b[39mUserPromptNode[AgentDepsT](\n\u001b[1;32m    333\u001b[0m         user_prompt\u001b[38;5;241m=\u001b[39muser_prompt,\n\u001b[1;32m    334\u001b[0m         system_prompts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system_prompts,\n\u001b[1;32m    335\u001b[0m         system_prompt_functions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system_prompt_functions,\n\u001b[1;32m    336\u001b[0m         system_prompt_dynamic_functions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system_prompt_dynamic_functions,\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# Actually run\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     end_result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    341\u001b[0m         start_node,\n\u001b[1;32m    342\u001b[0m         state\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m    343\u001b[0m         deps\u001b[38;5;241m=\u001b[39mgraph_deps,\n\u001b[1;32m    344\u001b[0m         infer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Build final run result\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# We don't do any advanced checking if the data is actually from a final result or not\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mRunResult(\n\u001b[1;32m    350\u001b[0m     state\u001b[38;5;241m.\u001b[39mmessage_history,\n\u001b[1;32m    351\u001b[0m     new_message_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m     state\u001b[38;5;241m.\u001b[39musage,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/pydantic_graph/graph.py:187\u001b[0m, in \u001b[0;36mGraph.run\u001b[0;34m(self, start_node, state, deps, infer_name)\u001b[0m\n\u001b[1;32m    185\u001b[0m next_node \u001b[38;5;241m=\u001b[39m start_node\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(next_node, history, state\u001b[38;5;241m=\u001b[39mstate, deps\u001b[38;5;241m=\u001b[39mdeps, infer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_node, End):\n\u001b[1;32m    189\u001b[0m         history\u001b[38;5;241m.\u001b[39mappend(EndStep(result\u001b[38;5;241m=\u001b[39mnext_node))\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/pydantic_graph/graph.py:263\u001b[0m, in \u001b[0;36mGraph.next\u001b[0;34m(self, node, history, state, deps, infer_name)\u001b[0m\n\u001b[1;32m    261\u001b[0m     start_ts \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mnow_utc()\n\u001b[1;32m    262\u001b[0m     start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 263\u001b[0m     next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m node\u001b[38;5;241m.\u001b[39mrun(ctx)\n\u001b[1;32m    264\u001b[0m     duration \u001b[38;5;241m=\u001b[39m perf_counter() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    266\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    267\u001b[0m     NodeStep(state\u001b[38;5;241m=\u001b[39mstate, node\u001b[38;5;241m=\u001b[39mnode, start_ts\u001b[38;5;241m=\u001b[39mstart_ts, duration\u001b[38;5;241m=\u001b[39mduration, snapshot_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnapshot_state)\n\u001b[1;32m    268\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/pydantic_ai/_agent_graph.py:254\u001b[0m, in \u001b[0;36mModelRequestNode.run\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    252\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m merge_model_settings(ctx\u001b[38;5;241m.\u001b[39mdeps\u001b[38;5;241m.\u001b[39mmodel_settings, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _logfire\u001b[38;5;241m.\u001b[39mspan(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel request\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[0;32m--> 254\u001b[0m     model_response, request_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mdeps\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    255\u001b[0m         ctx\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmessage_history, model_settings, model_request_parameters\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m     span\u001b[38;5;241m.\u001b[39mset_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m, model_response)\n\u001b[1;32m    258\u001b[0m     span\u001b[38;5;241m.\u001b[39mset_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m, request_usage)\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/pydantic_ai/models/groq.py:130\u001b[0m, in \u001b[0;36mGroqModel.request\u001b[0;34m(self, messages, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    125\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[ModelMessage],\n\u001b[1;32m    126\u001b[0m     model_settings: ModelSettings \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m     model_request_parameters: ModelRequestParameters,\n\u001b[1;32m    128\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ModelResponse, usage\u001b[38;5;241m.\u001b[39mUsage]:\n\u001b[1;32m    129\u001b[0m     check_allow_model_requests()\n\u001b[0;32m--> 130\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_completions_create(\n\u001b[1;32m    131\u001b[0m         messages, \u001b[38;5;28;01mFalse\u001b[39;00m, cast(GroqModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(response), _map_usage(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/pydantic_ai/models/groq.py:197\u001b[0m, in \u001b[0;36mGroqModel._completions_create\u001b[0;34m(self, messages, stream, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    193\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    195\u001b[0m groq_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chain(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_message(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages)))\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    198\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name),\n\u001b[1;32m    199\u001b[0m     messages\u001b[38;5;241m=\u001b[39mgroq_messages,\n\u001b[1;32m    200\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    201\u001b[0m     parallel_tool_calls\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    202\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    203\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39mtool_choice \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    204\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    205\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    206\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    207\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    208\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    209\u001b[0m     seed\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    210\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    211\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    212\u001b[0m     logit_bias\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    213\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/groq/resources/chat/completions.py:649\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    525\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m    527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    651\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m    652\u001b[0m             {\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    677\u001b[0m             },\n\u001b[1;32m    678\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    679\u001b[0m         ),\n\u001b[1;32m    680\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    681\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    682\u001b[0m         ),\n\u001b[1;32m    683\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    684\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    685\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m    686\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/groq/_base_client.py:1818\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1806\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1813\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1814\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1815\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1816\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1817\u001b[0m     )\n\u001b[0;32m-> 1818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/groq/_base_client.py:1526\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1524\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1527\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1528\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1529\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1530\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1531\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1532\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/site-packages/groq/_base_client.py:1612\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1611\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1613\u001b[0m         input_options,\n\u001b[1;32m   1614\u001b[0m         cast_to,\n\u001b[1;32m   1615\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1616\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1617\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1618\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/SaProt/lib/python3.10/asyncio/tasks.py:605\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    601\u001b[0m h \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcall_later(delay,\n\u001b[1;32m    602\u001b[0m                     futures\u001b[38;5;241m.\u001b[39m_set_result_unless_cancelled,\n\u001b[1;32m    603\u001b[0m                     future, result)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m     h\u001b[38;5;241m.\u001b[39mcancel()\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "for gt_key, gt_value in ground_truth.items():\n",
    "    print(f\"Groundtruth {gt_key}: {gt_value}\")\n",
    "    \n",
    "    for model in model_outputs.keys():\n",
    "        auto_value = model_outputs[model].get(gt_key, \"N/A\")  # Handle missing keys safely\n",
    "\n",
    "        user_prompt = f\"\"\"manually annotated {gt_key}: {gt_value}\n",
    "automatically annotated {gt_key}: {auto_value}\"\"\"\n",
    "\n",
    "        try:\n",
    "            result = await validation_agent.run(user_prompt)  # Ensure `await` is used correctly\n",
    "            print(f\"Model {model}: {auto_value} \\t Accurate output: {result.data.are_equal}\")\n",
    "            print(f\"Justification: {result.data.justification}\")  # Print the justification\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing model {model} for key {gt_key}: {e}\")\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prop in list(props)[0:3]:\n",
    "    print(\"prop\")\n",
    "    for model in models:\n",
    "        user_prompt = f\"\"\" manually annotated {prop}: {groun_truth[prop]}\n",
    "        automatically annotated {prop}:  {getattr(eval(model),  prop)}\n",
    "        \"\"\"\n",
    "        print(\"\\n\\n\"+user_prompt)\n",
    "        r = await validation_agent.run(user_prompt)\n",
    "        print(r.data)\n",
    "\n",
    "        print(f\"{prop}:{getattr(eval(model),  prop)}:{r.data.are_equal}\", end=\"\\t\")\n",
    "    print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IGNORE THE FOLLOWING CODE, IT IS A WORK SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking manual results versus automatically extracted results\n",
    "## Setting class for boolean output based on intepretation if results are equal\n",
    "\n",
    "\n",
    "# ## Setting the prompt and model for validation agent\n",
    "# validation_agent = Agent(\n",
    "#     model=\"groq:llama-3.3-70b-specdec\",\n",
    "#     result_type=AreAnntoationsEqual,\n",
    "#     system_prompt = \"\"\"You are an expert taxonomist. You are comparing the outcome of a manually extracted result versus an automatically extracted result. You need to compare the automatic results and determine whether the result is synonymous or equal the manual one; taking into consideration\n",
    "#     linguisitc and formatting nuances. Your answer is whether the two results are similar True/False and a justificaiton for your answer\"\"\",\n",
    "# )\n",
    "\n",
    "# ## Models to compare\n",
    "# # models = [\"gpt_4o_output\", \"gpt_mini_output\", \"sonnet_output\", \"deepseek_llama\"]\n",
    "# models = [\"gpt_4o_output\", \"gpt_mini_output\", \"sonnet_output\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Column Names/ properties\n",
    "# props = HawaiianPlant.model_json_schema()['properties'].keys()\n",
    "# props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prop in list(props)[0:3]:\n",
    "    print(\"prop\")\n",
    "    for model in models:\n",
    "        user_prompt = f\"\"\" manually annotated {prop}: {groun_truth[prop]}\n",
    "        automatically annotated {prop}:  {getattr(eval(model),  prop)}\n",
    "        \"\"\"\n",
    "        print(\"\\n\\n\"+user_prompt)\n",
    "        r = await validation_agent.run(user_prompt)\n",
    "        print(r.data)\n",
    "\n",
    "        print(f\"{prop}:{getattr(eval(model),  prop)}:{r.data.are_equal}\", end=\"\\t\")\n",
    "    print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_df = pd.read_csv(\"/Users/williamharrigan/Desktop/UH/Year_3/semester_2/wagner/auto_extract_2.csv\")\n",
    "# auto_df.columns = auto_df.columns.str.lower()\n",
    "# auto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>common_name</th>\n",
       "      <th>hawaiian_name_1</th>\n",
       "      <th>hawaiian_name_2</th>\n",
       "      <th>hawaiian_name_3</th>\n",
       "      <th>hawaiian_name_4</th>\n",
       "      <th>wagner_pg_number</th>\n",
       "      <th>description</th>\n",
       "      <th>...</th>\n",
       "      <th>island_type_ma</th>\n",
       "      <th>island_type_kah</th>\n",
       "      <th>island_type_mo</th>\n",
       "      <th>island_type_l</th>\n",
       "      <th>island_type_o</th>\n",
       "      <th>island_type_kau</th>\n",
       "      <th>island_type_ni</th>\n",
       "      <th>island_type_a</th>\n",
       "      <th>fedstatus_t1 (do at end)</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asteraceae</td>\n",
       "      <td>Ambrosia</td>\n",
       "      <td>artemisiifolia</td>\n",
       "      <td>common ragweed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pg 256-257</td>\n",
       "      <td>Dicots</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NS</td>\n",
       "      <td>Naturalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asteraceae</td>\n",
       "      <td>Dubautia</td>\n",
       "      <td>laxa</td>\n",
       "      <td>na`ena`e pua melemele</td>\n",
       "      <td>Na`ena`e pua melemele</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pg 292-295,301</td>\n",
       "      <td>Dicots</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NS</td>\n",
       "      <td>Endemic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asteraceae</td>\n",
       "      <td>Tetramolopium</td>\n",
       "      <td>filiforme</td>\n",
       "      <td>no common name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pg 361-362, 365, 366</td>\n",
       "      <td>Dicots</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>Endemic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asteraceae</td>\n",
       "      <td>Encelia</td>\n",
       "      <td>farinosa</td>\n",
       "      <td>brittle bush</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pg 312-313</td>\n",
       "      <td>Dicots</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NS</td>\n",
       "      <td>Naturalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aristolochiaceae</td>\n",
       "      <td>Aristolochia</td>\n",
       "      <td>littoralis</td>\n",
       "      <td>calico flower</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pg 237-238,239</td>\n",
       "      <td>Dicots</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NS</td>\n",
       "      <td>Naturalized</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             family          genus         species            common_name  \\\n",
       "0        Asteraceae       Ambrosia  artemisiifolia         common ragweed   \n",
       "1        Asteraceae       Dubautia            laxa  na`ena`e pua melemele   \n",
       "2        Asteraceae  Tetramolopium       filiforme         no common name   \n",
       "3        Asteraceae        Encelia        farinosa           brittle bush   \n",
       "4  Aristolochiaceae   Aristolochia      littoralis          calico flower   \n",
       "\n",
       "         hawaiian_name_1 hawaiian_name_2 hawaiian_name_3 hawaiian_name_4  \\\n",
       "0                    NaN             NaN             NaN             NaN   \n",
       "1  Na`ena`e pua melemele             NaN             NaN             NaN   \n",
       "2                    NaN             NaN             NaN             NaN   \n",
       "3                    NaN             NaN             NaN             NaN   \n",
       "4                    NaN             NaN             NaN             NaN   \n",
       "\n",
       "       wagner_pg_number description  ... island_type_ma  island_type_kah  \\\n",
       "0            pg 256-257      Dicots  ...            1.0              0.0   \n",
       "1        pg 292-295,301      Dicots  ...            0.0              0.0   \n",
       "2  pg 361-362, 365, 366      Dicots  ...            0.0              0.0   \n",
       "3            pg 312-313      Dicots  ...            1.0              0.0   \n",
       "4        pg 237-238,239      Dicots  ...            1.0              0.0   \n",
       "\n",
       "   island_type_mo  island_type_l  island_type_o  island_type_kau  \\\n",
       "0             1.0            0.0            1.0              0.0   \n",
       "1             0.0            0.0            1.0              0.0   \n",
       "2             0.0            0.0            1.0              0.0   \n",
       "3             0.0            0.0            0.0              0.0   \n",
       "4             0.0            0.0            1.0              1.0   \n",
       "\n",
       "   island_type_ni  island_type_a  fedstatus_t1 (do at end)        status  \n",
       "0             0.0            0.0                         NS  Naturalized  \n",
       "1             0.0            0.0                         NS      Endemic  \n",
       "2             0.0            0.0                          E      Endemic  \n",
       "3             0.0            0.0                         NS  Naturalized  \n",
       "4             0.0            0.0                         NS  Naturalized  \n",
       "\n",
       "[5 rows x 626 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"/Users/williamharrigan/Desktop/Github/ai_wagner_trait_data_extraction/files/man_extract.csv\")\n",
    "df.columns = df.columns.str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem_height_m_exmin</th>\n",
       "      <th>stem_height_m_min</th>\n",
       "      <th>stem_height_m_max</th>\n",
       "      <th>stem_height_m_exmax</th>\n",
       "      <th>stem_hair_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stem_height_m_exmin  stem_height_m_min  stem_height_m_max  \\\n",
       "47                  NaN                6.0               12.0   \n",
       "48                  NaN                6.0                8.0   \n",
       "49                  NaN                5.0               10.0   \n",
       "\n",
       "    stem_height_m_exmax stem_hair_type  \n",
       "47                  NaN            NaN  \n",
       "48                  NaN            NaN  \n",
       "49                  NaN            NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['family'] == 'Agavaceae') & (df['genus'] == 'Pleomele')].filter(like='stem')\n",
    "# amp[amp['family'] == 'Brassicaceae']['leaflet_leaf_width'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan,\n",
       "        {'exmin': nan, 'min': 6.0, 'max': 12.0, 'exmax': nan, 'unit': 'm'}],\n",
       "       [nan,\n",
       "        {'exmin': nan, 'min': 6.0, 'max': 8.0, 'exmax': nan, 'unit': 'm'}],\n",
       "       [nan,\n",
       "        {'exmin': nan, 'min': 5.0, 'max': 10.0, 'exmax': nan, 'unit': 'm'}]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[(new_df['family'] == 'Agavaceae') & (new_df['genus'] == 'Pleomele')].filter(like='stem').values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_type_columns(df):\n",
    "    # Identify all columns that have '_type_' in their names\n",
    "    type_cols = [col for col in df.columns if '_type_' in col]\n",
    "\n",
    "    # Group columns by their prefix (everything before '_type_' + last element)\n",
    "    col_groups = {}\n",
    "    for col in type_cols:\n",
    "        prefix = '_'.join(col.split('_')[:-1])  # Get the prefix\n",
    "        if prefix not in col_groups:\n",
    "            col_groups[prefix] = []\n",
    "        col_groups[prefix].append(col)\n",
    "\n",
    "    # Create new collapsed columns\n",
    "    for prefix, cols in col_groups.items():\n",
    "        df[prefix] = df.apply(lambda row: [col.split('_')[-1].upper() for col in cols if row[col] == 1], axis=1)\n",
    "        \n",
    "    # Drop the original type columns\n",
    "    df.drop(columns=type_cols, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_hawaiian_name_columns(df):\n",
    "    # Identify columns that contain 'hawaiian_name'\n",
    "    hawaiian_cols = [col for col in df.columns if 'hawaiian_name' in col]\n",
    "\n",
    "    # Ensure there are columns to merge\n",
    "    if not hawaiian_cols:\n",
    "        return df\n",
    "    \n",
    "    # Merge values into a list, ensuring all values are strings and filtering out empty values\n",
    "    df['hawaiian_name'] = df[hawaiian_cols].apply(lambda row: [str(val) for val in row if pd.notna(val) and str(val).strip() != ''], axis=1)\n",
    "\n",
    "    # Drop the original columns\n",
    "    df.drop(columns=hawaiian_cols, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_cols = [col for col in df.columns if '_type_' in col]\n",
    "# type_cols\n",
    "# Group columns by their prefix (everything before '_type_' + last element)\n",
    "col_groups = {}\n",
    "for col in type_cols:\n",
    "    prefix = '_'.join(col.split('_')[:-1])\n",
    "    # print(prefix)\n",
    "    if prefix not in col_groups:\n",
    "        col_groups[prefix] = []\n",
    "    col_groups[prefix].append(col)\n",
    "    # print(col)\n",
    "\n",
    "# # Create new collapsed columns\n",
    "for prefix, cols in col_groups.items():\n",
    "    df[prefix] = df.apply(lambda row: [col.split('_')[-1].upper() for col in cols if row[col] == 1], axis=1)\n",
    "    \n",
    "# Drop the original type columns\n",
    "df.drop(columns=type_cols, inplace=True)\n",
    "\n",
    "df['hawaiian_name'] = df[['hawaiian_name_1', 'hawaiian_name_2', 'hawaiian_name_3', 'hawaiian_name_4']].apply(\n",
    "    lambda row: {x for x in row if pd.notna(x)}, axis=1\n",
    ")\n",
    "\n",
    "# Drop original columns if needed\n",
    "df.drop(columns=['hawaiian_name_1', 'hawaiian_name_2', 'hawaiian_name_3', 'hawaiian_name_4'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# hawaiian_cols = [col for col in df.columns if 'hawaiian_name' in col]\n",
    "# # Merge values into a list, ensuring all values are strings and filtering out empty values\n",
    "# df['hawaiian_name'] = df[hawaiian_cols].apply(lambda row: [str(val) for val in row if pd.notna(val) and str(val).strip() != ''], axis=1)\n",
    "\n",
    "# # Drop the original columns\n",
    "# df.drop(columns=hawaiian_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [H, MA, MO, O]\n",
       "1                [O]\n",
       "2                [O]\n",
       "3               [MA]\n",
       "4       [MA, O, KAU]\n",
       "           ...      \n",
       "94                []\n",
       "95                []\n",
       "96                []\n",
       "97                []\n",
       "98          [O, KAU]\n",
       "Name: island_type, Length: 99, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['island_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_measurements(df):\n",
    "    # Group columns by their base name (everything before _min, _max, etc.)\n",
    "    base_columns = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        parts = col.split('_')\n",
    "        if len(parts) > 2:\n",
    "            suffix = parts[-1]\n",
    "            if suffix in ['exmin', 'min', 'max', 'exmax']:\n",
    "                base_name = '_'.join(parts[:-1])\n",
    "                if base_name not in base_columns:\n",
    "                    base_columns[base_name] = []\n",
    "                base_columns[base_name].append(col)\n",
    "    \n",
    "    # Columns to copy directly (non-measurement columns)\n",
    "    non_measurement_cols = [col for col in df.columns \n",
    "                           if not any(col in cols for cols in base_columns.values())]\n",
    "    \n",
    "    # Start with non-measurement columns\n",
    "    result_df = df[non_measurement_cols].copy()\n",
    "    \n",
    "    # Process each group of measurement columns\n",
    "    collapsed_data = {}\n",
    "    \n",
    "    for base_name, cols in base_columns.items():\n",
    "        # Extract just the name without the unit\n",
    "        name_parts = base_name.split('_')\n",
    "        unit = name_parts[-1]\n",
    "        col_name = '_'.join(name_parts[:-1])\n",
    "        \n",
    "        # Create dictionaries for each row\n",
    "        collapsed_data[col_name] = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            measurement_dict = {\n",
    "                'exmin': row[f\"{base_name}_exmin\"] if f\"{base_name}_exmin\" in cols else None,\n",
    "                'min': row[f\"{base_name}_min\"] if f\"{base_name}_min\" in cols else None,\n",
    "                'max': row[f\"{base_name}_max\"] if f\"{base_name}_max\" in cols else None,\n",
    "                'exmax': row[f\"{base_name}_exmax\"] if f\"{base_name}_exmax\" in cols else None,\n",
    "                'unit': unit\n",
    "            }\n",
    "            collapsed_data[col_name].append(measurement_dict)\n",
    "    \n",
    "    # Convert to DataFrame and concatenate with result_df\n",
    "    collapsed_df = pd.DataFrame(collapsed_data)\n",
    "    result_df = pd.concat([result_df, collapsed_df], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage\n",
    "# df is your original dataframe with columns like stem_height_m_exmin, etc.\n",
    "new_df = collapse_measurements(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = collapse_measurements(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family\n",
      "genus\n",
      "species\n",
      "common_name\n",
      "wagner_pg_number\n",
      "description\n",
      "infraspecific_epithet\n",
      "stem_hair_type\n",
      "phyllotaxy_type\n",
      "leaf_hair_description\n",
      "leaf_hair_upper_description\n",
      "leaf_hair_lower_description\n",
      "breeding_type\n",
      "inflorescence_type\n",
      "ray_color\n",
      "floret_color\n",
      "spathe_color\n",
      "perianth_outer_color\n",
      "perianth_inner_color\n",
      "perianth_color\n",
      "labellum_color\n",
      "corolla_type\n",
      "staminate_corolla_type\n",
      "pistillate_corolla_type\n",
      "corolla_color\n",
      "fruit_type\n",
      "fruit_width_mm_max.1\n",
      "fruit_diameter_mm_min.1\n",
      "ploidy\n",
      "chromosome_#\n",
      "average_chromosome_#\n",
      "origin_t1\n",
      "fedstatus_t1 (do at end) \n",
      "status\n",
      "life_form_type\n",
      "leaf_type\n",
      "leaf_margin_type\n",
      "leaf_shape_type\n",
      "juvenile_leaf_type\n",
      "juvenile_leaf_margin_type\n",
      "juvenile_leaf_shape_type\n",
      "leaflets_leaf_type\n",
      "leaflets_leaf_margin_type\n",
      "leaflets_leaf_shape_type\n",
      "leaf_hair_type\n",
      "leaf_hair_upper_type\n",
      "leaf_hair_lower_type\n",
      "juvenile_leaf_hair_type\n",
      "island_type\n",
      "hawaiian_name\n",
      "stem_height\n",
      "leaf_length\n",
      "leaf_width\n",
      "juvenile_leaf_length\n",
      "juvenile_leaf_width\n",
      "leaflet_leaf_length\n",
      "leaflet_leaf_width\n",
      "petioles\n",
      "staminate_inflorescence_length\n",
      "staminate_inflorescence_width\n",
      "pistillate_inflorescence_length\n",
      "pistillate_inflorescence_width\n",
      "inflorescence_flower_length\n",
      "inflorescence_flower_width\n",
      "flower_length\n",
      "flower_width\n",
      "rachis_length\n",
      "rachis_diameter\n",
      "head_length\n",
      "head_diameter\n",
      "bur_length\n",
      "tepal_length\n",
      "staminate_tepal_length\n",
      "pistillate_tepal_length\n",
      "ray_length\n",
      "ray_width\n",
      "florets_length\n",
      "involucre_length\n",
      "involucre_width\n",
      "staminate_involucre_length\n",
      "pistilate_involucre_length\n",
      "bract_length\n",
      "bract_width\n",
      "bract_lower_length\n",
      "bract_outer_length\n",
      "bracteoles_length\n",
      "bracteoles_width\n",
      "pedicel_length\n",
      "pedicel_width\n",
      "staminate_pedicel_width\n",
      "staminate_ppedicel_width\n",
      "pistillate_pedicel_width\n",
      "pistillate_pedicel_length\n",
      "hypanthium_length\n",
      "hypanthium_width\n",
      "peduncle_length\n",
      "peduncle_width\n",
      "staminate_peduncle_length\n",
      "staminate_peduncle_width\n",
      "pistillate_peduncle_length\n",
      "pistillate_peduncle_width\n",
      "spathe_length\n",
      "spathe_width\n",
      "spadix_length\n",
      "perianth_length\n",
      "perianth_width\n",
      "perianthl_length_outer\n",
      "perianthl_width_outer\n",
      "perianthl_length_inner\n",
      "perianthl_width_inner\n",
      "perianth_tube_length\n",
      "perianth_lobes_length\n",
      "perianth_lobes_width\n",
      "staminate_perianth_tube_length\n",
      "pistillate_perianth_tube_length\n",
      "pappus_length\n",
      "umbellet_length\n",
      "labellum_length\n",
      "labellum_width\n",
      "calyx_length\n",
      "calyx_width\n",
      "calyx_teeth_length\n",
      "calyx_teeth_width\n",
      "calyx_length_lobes\n",
      "calyx_width_lobes\n",
      "upper_calyx_length\n",
      "lower_calyx_length\n",
      "calyx_lobes_length_inner\n",
      "calyx_lobes_width_inner\n",
      "calyx_lobes_length_outer\n",
      "calyx_lobes_width_outer\n",
      "calyx_tube_length\n",
      "calyx_tube_width\n",
      "male_calyx_length\n",
      "male_calyx_width\n",
      "male_calyx_length_lobes\n",
      "male_calyx_width_lobes\n",
      "female_calyx _length\n",
      "female_calyx _width\n",
      "female_calyx_length_lobes\n",
      "female_calyx_width_lobes\n",
      "male_calyx_lobes_length_inner\n",
      "male_calyx_lobes_length_outer\n",
      "male_calyx_lobes_width_outer\n",
      "male_calyx_tube_length\n",
      "female_calyx_lobes_length_inner\n",
      "female_calyx_lobes_length_outer\n",
      "female_calyx_lobes_width_inner\n",
      "female_calyx_lobes_width_outer\n",
      "female_calyx_length\n",
      "inner_calyx_length\n",
      "outer_calyx_length\n",
      "corolla_length\n",
      "corolla_width\n",
      "corolla_tube_length\n",
      "corolla_tube_width\n",
      "corolla_lobes_length\n",
      "corolla_lobes_width\n",
      "upper_corolla\n",
      "lower_corolla\n",
      "upper_corolla_lobes_length\n",
      "lower_corolla_lobes_length\n",
      "corolla_lip\n",
      "staminate_corolla_length\n",
      "pistillate_corolla_length\n",
      "staminate_corolla_tube_length\n",
      "staminate_corolla_tube_width\n",
      "pistillate_corolla_tube_length\n",
      "pistillate_corolla_tube_width\n",
      "female_corolla_lobes_length\n",
      "female_corolla_lobes_width\n",
      "male_corrola_lobes_length\n",
      "male_corrola_lobes_width\n",
      "fruit_length\n",
      "fruit_width\n",
      "fruit_diameter\n",
      "seeds\n",
      "seed_length\n",
      "seed_width\n",
      "seed_diameter\n"
     ]
    }
   ],
   "source": [
    "for i in new_df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"/Users/williamharrigan/Desktop/Github/ai_wagner_trait_data_extraction/files/man_extract.csv\")\n",
    "df.columns = df.columns.str.lower()\n",
    "df.head()\n",
    "type_cols = [col for col in df.columns if '_type_' in col]\n",
    "# type_cols\n",
    "# Group columns by their prefix (everything before '_type_' + last element)\n",
    "col_groups = {}\n",
    "for col in type_cols:\n",
    "    prefix = '_'.join(col.split('_')[:-1])\n",
    "    # print(prefix)\n",
    "    if prefix not in col_groups:\n",
    "        col_groups[prefix] = []\n",
    "    col_groups[prefix].append(col)\n",
    "    # print(col)\n",
    "\n",
    "# # Create new collapsed columns\n",
    "for prefix, cols in col_groups.items():\n",
    "    df[prefix] = df.apply(lambda row: [col.split('_')[-1].upper() for col in cols if row[col] == 1], axis=1)\n",
    "    \n",
    "# Drop the original type columns\n",
    "df.drop(columns=type_cols, inplace=True)\n",
    "\n",
    "df['hawaiian_name'] = df[['hawaiian_name_1', 'hawaiian_name_2', 'hawaiian_name_3', 'hawaiian_name_4']].apply(\n",
    "    lambda row: {x for x in row if pd.notna(x)}, axis=1\n",
    ")\n",
    "\n",
    "# Drop original columns if needed\n",
    "df.drop(columns=['hawaiian_name_1', 'hawaiian_name_2', 'hawaiian_name_3', 'hawaiian_name_4'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# hawaiian_cols = [col for col in df.columns if 'hawaiian_name' in col]\n",
    "# # Merge values into a list, ensuring all values are strings and filtering out empty values\n",
    "# df['hawaiian_name'] = df[hawaiian_cols].apply(lambda row: [str(val) for val in row if pd.notna(val) and str(val).strip() != ''], axis=1)\n",
    "\n",
    "# # Drop the original columns\n",
    "# df.drop(columns=hawaiian_cols, inplace=True)\n",
    "new_df = collapse_measurements(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = new_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>common_name</th>\n",
       "      <th>wagner_pg_number</th>\n",
       "      <th>description</th>\n",
       "      <th>infraspecific_epithet</th>\n",
       "      <th>stem_hair_type</th>\n",
       "      <th>phyllotaxy_type</th>\n",
       "      <th>leaf_hair_description</th>\n",
       "      <th>...</th>\n",
       "      <th>female_corolla_lobes_width</th>\n",
       "      <th>male_corrola_lobes_length</th>\n",
       "      <th>male_corrola_lobes_width</th>\n",
       "      <th>fruit_length</th>\n",
       "      <th>fruit_width</th>\n",
       "      <th>fruit_diameter</th>\n",
       "      <th>seeds</th>\n",
       "      <th>seed_length</th>\n",
       "      <th>seed_width</th>\n",
       "      <th>seed_diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Brassicaceae</td>\n",
       "      <td>Nasturtium</td>\n",
       "      <td>microphyllum</td>\n",
       "      <td>watercress</td>\n",
       "      <td>pg 411</td>\n",
       "      <td>Dicots</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>glabrous</td>\n",
       "      <td>...</td>\n",
       "      <td>{'exmin': nan, 'min': nan, 'max': nan, 'exmax'...</td>\n",
       "      <td>{'exmin': nan, 'min': nan, 'max': nan, 'exmax'...</td>\n",
       "      <td>{'exmin': nan, 'min': nan, 'max': nan, 'exmax'...</td>\n",
       "      <td>{'exmin': nan, 'min': 16.0, 'max': 25.0, 'exma...</td>\n",
       "      <td>{'exmin': nan, 'min': nan, 'max': nan, 'exmax'...</td>\n",
       "      <td>{'exmin': nan, 'min': nan, 'max': nan, 'exmax'...</td>\n",
       "      <td>{'exmin': None, 'min': None, 'max': nan, 'exma...</td>\n",
       "      <td>{'exmin': nan, 'min': nan, 'max': nan, 'exmax'...</td>\n",
       "      <td>{'exmin': None, 'min': nan, 'max': nan, 'exmax...</td>\n",
       "      <td>{'exmin': None, 'min': nan, 'max': nan, 'exmax...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          family       genus       species common_name wagner_pg_number  \\\n",
       "28  Brassicaceae  Nasturtium  microphyllum  watercress           pg 411   \n",
       "\n",
       "   description infraspecific_epithet stem_hair_type phyllotaxy_type  \\\n",
       "28      Dicots                   NaN              G               A   \n",
       "\n",
       "   leaf_hair_description  ...  \\\n",
       "28              glabrous  ...   \n",
       "\n",
       "                           female_corolla_lobes_width  \\\n",
       "28  {'exmin': nan, 'min': nan, 'max': nan, 'exmax'...   \n",
       "\n",
       "                            male_corrola_lobes_length  \\\n",
       "28  {'exmin': nan, 'min': nan, 'max': nan, 'exmax'...   \n",
       "\n",
       "                             male_corrola_lobes_width  \\\n",
       "28  {'exmin': nan, 'min': nan, 'max': nan, 'exmax'...   \n",
       "\n",
       "                                         fruit_length  \\\n",
       "28  {'exmin': nan, 'min': 16.0, 'max': 25.0, 'exma...   \n",
       "\n",
       "                                          fruit_width  \\\n",
       "28  {'exmin': nan, 'min': nan, 'max': nan, 'exmax'...   \n",
       "\n",
       "                                       fruit_diameter  \\\n",
       "28  {'exmin': nan, 'min': nan, 'max': nan, 'exmax'...   \n",
       "\n",
       "                                                seeds  \\\n",
       "28  {'exmin': None, 'min': None, 'max': nan, 'exma...   \n",
       "\n",
       "                                          seed_length  \\\n",
       "28  {'exmin': nan, 'min': nan, 'max': nan, 'exmax'...   \n",
       "\n",
       "                                           seed_width  \\\n",
       "28  {'exmin': None, 'min': nan, 'max': nan, 'exmax...   \n",
       "\n",
       "                                        seed_diameter  \n",
       "28  {'exmin': None, 'min': nan, 'max': nan, 'exmax...  \n",
       "\n",
       "[1 rows x 180 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp[amp['family'] == 'Brassicaceae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'leaflet_leaf_length' in amp[amp['family'] == 'Brassicaceae'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv('/Users/williamharrigan/Desktop/test_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in new_df.columns:\n",
    "#     if len(i.split('_')) > 2:\n",
    "#         thing = metric = i.split('_')[-1]\n",
    "#         metric = i.split('_')[-2]\n",
    "#         col_name = '_'.join(i.split('_')[:-2])\n",
    "#         if thing == 'max' or thing == 'min' or thing == 'exmax' or thing == 'exmin':\n",
    "#             print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SaProt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
